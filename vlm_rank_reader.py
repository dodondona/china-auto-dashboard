#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
vlm_rank_reader.py
- /rank/1 ã‚’é–‹ã„ã¦ãƒ•ãƒ«ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ã‚·ãƒ§(ã‚¿ã‚¤ãƒ«åˆ†å‰²)ã‚’ä½œæˆ
- ç”»åƒã‚’ VLM (OpenAI gpt-4o / gpt-4o-mini) ã«æ¸¡ã—ã¦è¡¨ãƒ‡ãƒ¼ã‚¿(ranking rows)ã‚’æŠ½å‡º
- CSVã«ä¿å­˜

ä½¿ã„æ–¹ä¾‹:
  python vlm_rank_reader.py \
    --from-url https://www.autohome.com.cn/rank/1 \
    --out data/autohome_rank_2025-08.csv \
    --model gpt-4o-mini
"""

import os, io, re, csv, math, time, base64, json, argparse
from pathlib import Path

from playwright.sync_api import sync_playwright, TimeoutError
from openai import OpenAI

# ===== èª­ã¿å–ã‚Šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå…ƒã®ä»•æ§˜è¸è¥²ï¼‰ =====
SYSTEM_PROMPT = """ã‚ãªãŸã¯è¡¨ã®èª­ã¿å–ã‚Šã«ç‰¹åŒ–ã—ãŸè¦–è¦šã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ç”»åƒã¯ä¸­å›½ã®è‡ªå‹•è»Šè²©å£²ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ã™ã€‚UIéƒ¨å“ã‚„åºƒå‘Šã¯ç„¡è¦–ã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯ JSON ã®ã¿ã€‚æ§‹é€ :
{
  "rows": [
    {"rank": <int|null>, "name": "<string>", "count": <int|null>}
  ]
}
"""

UA_MOBILE = (
    "Mozilla/5.0 (Linux; Android 11; Pixel 5) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/122.0.0.0 Mobile Safari/537.36"
)

def b64_image(path: Path) -> str:
    return base64.b64encode(path.read_bytes()).decode("ascii")

def chunk_image_and_call_vlm(img_path: Path, client: OpenAI, model: str) -> list[dict]:
    """
    å¿…è¦ã«å¿œã˜ã¦ç¸¦åˆ†å‰²(ã‚¿ã‚¤ãƒ«)ã—ã¦è¤‡æ•°ç”»åƒã‚’ä¸€åº¦ã«æŠ•ã’ã‚‹ã€‚
    VLMã¸ã®æŠ•ã’æ–¹ã¯å…ƒãƒ­ã‚¸ãƒƒã‚¯è¸è¥²ã€‚æ¸©åº¦0ã§å®‰å®šåŒ–ã€‚
    """
    from PIL import Image
    im = Image.open(img_path)
    H = im.height
    MAX_SLICE = 2200               # 1æšã‚ãŸã‚Šã®é«˜ã•ä¸Šé™ï¼ˆå®‰å®šç”¨ï¼‰
    n = math.ceil(H / MAX_SLICE)
    imgs = []
    for i in range(n):
        top = i * MAX_SLICE
        bottom = min(H, (i+1)*MAX_SLICE)
        crop = im.crop((0, top, im.width, bottom))
        buf = io.BytesIO()
        crop.save(buf, format="PNG")
        imgs.append(base64.b64encode(buf.getvalue()).decode("ascii"))

    # ç”»åƒã‚’é †ã«é£Ÿã‚ã›ã‚‹
    messages = [
        {"role":"system","content": SYSTEM_PROMPT},
        {"role":"user","content": [{"type":"text","text":"æ¬¡ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆç¾¤ã‹ã‚‰è¡¨ã‚’èª­ã¿å–ã£ã¦ãã ã•ã„ã€‚"}] }
    ]
    for enc in imgs:
        messages[1]["content"].append({"type":"image_url","image_url":{"url": f"data:image/png;base64,{enc}"}})

    resp = client.chat.completions.create(
        model=model,
        temperature=0,
        max_tokens=1500,
        messages=messages,
    )
    txt = resp.choices[0].message.content.strip()
    m = re.search(r'\{[^]*"rows"\s*:\s*\[[\s\S]*?\][^}]*\}', txt)
    payload = json.loads(m.group(0)) if m else json.loads(txt)
    rows = payload.get("rows", [])
    # æ­£è¦åŒ–
    out = []
    for r in rows:
        try:
            rk = int(r.get("rank")) if r.get("rank") is not None else None
        except Exception:
            rk = None
        name = (r.get("name") or "").strip()
        try:
            cnt = int(str(r.get("count")).replace(",","")) if r.get("count") not in (None,"") else None
        except Exception:
            cnt = None
        if rk or name or cnt:
            out.append({"rank": rk, "name": name, "count": cnt})
    return out

def goto_with_retries(page, url: str, timeout_ms: int = 120000):
    """
    ã§ãã‚‹ã ã‘â€ä»Šã¾ã§é€šã‚Šâ€ã®æŒ™å‹•ã‚’ä¿ã¡ã¤ã¤ã€åˆ°é”æ€§ã ã‘å¼·åŒ–ã€‚
    - è¤‡æ•°å€™è£œURLã«ãƒªãƒˆãƒ©ã‚¤ï¼ˆwww/mï¼‰
    - wait_until=load / domcontentloaded ã‚’åˆ‡æ›¿
    """
    candidates = [
        (url, "load"),
        (url, "domcontentloaded"),
    ]
    # www â†’ m (è»½ã„UIã§é€Ÿã„ã“ã¨ãŒå¤šã„)
    if "autohome.com.cn/rank/1" in url:
        candidates += [
            ("https://m.autohome.com.cn/rank/1", "load"),
            ("https://m.autohome.com.cn/rank/1", "domcontentloaded"),
        ]

    last_err = None
    for u, wait in candidates:
        try:
            page.goto(u, wait_until=wait, timeout=timeout_ms)
            return u
        except TimeoutError as e:
            last_err = e
            page.wait_for_timeout(1200)
            continue
    # ã“ã“ã¾ã§å¤±æ•—ã—ãŸã‚‰ãã®ã¾ã¾ä¾‹å¤–
    raise last_err or TimeoutError("goto retries exhausted")

def scroll_to_bottom(page, idle_ms=700, max_rounds=60):
    """
    ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’â€å¢—ãˆãªããªã‚‹ã¾ã§Ã—é€£ç¶š3å›â€ã§çµ‚äº†ã€‚
    """
    prev = -1
    stable = 0
    for i in range(max_rounds):
        page.mouse.wheel(0, 20000)
        page.wait_for_timeout(idle_ms)
        n = page.evaluate("() => document.querySelectorAll('[data-rank-num]').length")
        if n == prev:
            stable += 1
        else:
            stable = 0
        prev = n
        if stable >= 3:
            break
    return prev

def capture_fullpage_screenshot(url: str, out_png: Path) -> int:
    """
    ç›´æ¥ã‚µã‚¤ãƒˆã‚’é–‹ãã€æœ€ä¸‹éƒ¨ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ãƒ•ãƒ«ãƒšãƒ¼ã‚¸ã®pngã‚’ä¿å­˜ã€‚
    è¿”ã‚Šå€¤: è¦‹ã¤ã‹ã£ãŸè¡Œæ•° (ç›®å®‰)
    """
    from playwright.sync_api import sync_playwright
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled"])
        ctx = browser.new_context(
            user_agent=UA_MOBILE,
            viewport={"width": 440, "height": 900},
            locale="zh-CN",
            timezone_id="Asia/Shanghai",
        )
        # è»½é‡åŒ–ï¼ˆç”»åƒã¾ã§ã¯ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ã€‚ãƒ•ã‚©ãƒ³ãƒˆ/è§£æç³»ã¯ãƒ–ãƒ­ãƒƒã‚¯ï¼‰
        ctx.route("**/*", lambda route: route.abort() if any(
            x in route.request.url for x in [
                "googletagmanager", "analytics", "gtag", "baidu.com/hm", "umeng", "heatmap"
            ]) else route.continue_())
        ctx.set_default_navigation_timeout(120000)
        page = ctx.new_page()

        reached = goto_with_retries(page, url, timeout_ms=120000)
        page.wait_for_load_state("networkidle")
        rows = scroll_to_bottom(page, idle_ms=700, max_rounds=60)

        # ãƒ¢ãƒã‚¤ãƒ«UIã¯ fixed ãƒ˜ãƒƒãƒ€ãŒé‡ãªã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ã€ä¸€ç¬ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã‹ã‚‰æ’®ã‚‹
        page.evaluate("() => window.scrollTo(0, 0)")
        page.wait_for_timeout(200)
        page.screenshot(path=str(out_png), full_page=True)
        browser.close()
        return rows

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--from-url", required=True)
    ap.add_argument("--out", required=True)
    ap.add_argument("--model", default="gpt-4o-mini")
    ap.add_argument("--screenshot", default="data/rank_fullpage.png")
    args = ap.parse_args()

    out_png = Path(args.screenshot)
    out_png.parent.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ“¥ navigate: {args.from_url}")
    rows_seen = capture_fullpage_screenshot(args.from_url, out_png)
    print(f"   rows_seenâ‰ˆ{rows_seen}, screenshot: {out_png}")

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚")
    client = OpenAI(api_key=api_key)

    print("ğŸ§  VLM èª­ã¿å–ã‚Šä¸­...")
    rows = chunk_image_and_call_vlm(out_png, client, args.model)

    # rankãŒãªã„/é£›ã‚“ã§ã„ã‚‹å ´åˆã¯è£œå®Œï¼ˆç”»åƒä¸Šã®é †ã§å†æ¡ç•ªï¼‰
    norm = []
    r_auto = 1
    for r in rows:
        rk = r["rank"] if r["rank"] else r_auto
        r_auto = rk + 1
        name = r["name"].strip()
        cnt = r["count"]
        norm.append({"rank": rk, "name": name, "count": cnt})

    # CSVå‡ºåŠ›
    Path(args.out).parent.mkdir(parents=True, exist_ok=True)
    with open(args.out, "w", encoding="utf-8-sig", newline="") as f:
        w = csv.DictWriter(f, fieldnames=["rank","name","count"])
        w.writeheader()
        w.writerows(sorted(norm, key=lambda x: x["rank"]))

    print(f"âœ… saved: {args.out}")

if __name__ == "__main__":
    main()
