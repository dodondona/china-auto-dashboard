name: autohome_config_to_csv

on:
  workflow_dispatch:
  schedule:
    - cron: '15 1 * * *'
  workflow_run:
    workflows:
      - autohome_pipeline
    types:
      - completed

permissions:
  contents: read
  actions: read

jobs:
  autohome_config_to_csv:
    if: ${{ github.event_name != 'workflow_run' || github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python deps (lenient)
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt || true; fi
          pip install requests || true

      - name: Download series_list.csv from autohome_pipeline (same repo latest run)
        if: ${{ github.event_name == 'workflow_run' }}
        uses: actions/download-artifact@v4
        with:
          name: autohome-series-list
          path: public

      - name: Fallback: build series_list.csv from live ranking page (top 100)
        if: ${{ !hashFiles('public/series_list.csv') }}
        run: |
          python - <<'PY'
          import re,csv,sys,os,requests
          os.makedirs('public', exist_ok=True)
          headers = {
              'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122 Safari/537.36',
              'Accept-Language':'zh-CN,zh;q=0.9,en;q=0.8',
              'Referer':'https://www.autohome.com.cn/rank/1'
          }
          try:
              html = requests.get('https://www.autohome.com.cn/rank/1', headers=headers, timeout=30).text
          except Exception as e:
              print('Failed to fetch ranking page:', e, file=sys.stderr)
              html = ''
          ids = re.findall(r'https?://www\.autohome\.com\.cn/(\d+)', html)
          seen=set(); uniq=[]
          for sid in ids:
              if sid not in seen:
                  seen.add(sid); uniq.append(sid)
          uniq = sorted(uniq, key=lambda x:int(x))[:100]
          with open('public/series_list.csv','w', newline='', encoding='utf-8') as f:
              w=csv.writer(f); w.writerow(['series_id','series_url','config_url'])
              for sid in uniq:
                  w.writerow([sid, f'https://www.autohome.com.cn/{sid}', f'https://www.autohome.com.cn/config/series/{sid}.html#pvareaid=3454437'])
          print(f'Fallback wrote {len(uniq)} rows to public/series_list.csv')
          PY

      - name: Run autohome_config_to_csv.py for top 100 series
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          set -euo pipefail
          while IFS=, read -r sid surl curl; do
            if [ "$sid" = "series_id" ] || [ -z "${sid:-}" ]; then continue; fi
            echo "::group::Config scrape for series $sid"
            python tools/autohome_config_to_csv.py "${curl}"
            echo "::endgroup::"
          done < public/series_list.csv
