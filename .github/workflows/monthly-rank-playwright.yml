#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
translate_brand_model_llm.py
- 中国語のブランド/モデル名を Wikipedia -> (任意で)Wikidata -> (任意で)公式サイトCSE の順で解決
- 既存CSVに brand_ja / model_ja を付与して出力
"""

import os
import json
import time
import argparse
import pandas as pd
from tqdm import tqdm

# ========================= Wikipedia / Wikidata ==============================

def lookup_wikipedia(term: str):
    """zh Wikipediaから言語間リンク（ja優先、なければen）を取得"""
    import wikipediaapi
    # User-Agent を明示（WikimediaのUAポリシー対応）
    wiki = wikipediaapi.Wikipedia(
        language='zh',
        user_agent='china-auto-dashboard/1.0 (https://github.com/dodondona/china-auto-dashboard; contact: github-actions)'
    )
    p = wiki.page(term)
    if not p.exists():
        return None
    links = p.langlinks
    if 'ja' in links:
        return links['ja'].title
    if 'en' in links:
        return links['en'].title
    return None

def lookup_wikidata(term: str):
    """Wikidataで zh 検索→ja/enラベルを取得（ja優先）"""
    import requests
    try:
        s = requests.get(
            "https://www.wikidata.org/w/api.php",
            params={
                "action": "wbsearchentities",
                "language": "zh",
                "format": "json",
                "search": term,
                "type": "item",
                "limit": 5,
            },
            timeout=10,
        ).json()
        if not s.get("search"):
            return None
        qid = s["search"][0]["id"]
        ent = requests.get(
            "https://www.wikidata.org/wiki/Special:EntityData/{}.json".format(qid),
            timeout=10,
        ).json()["entities"][qid]
        labels = ent.get("labels", {})
        return labels.get("ja", labels.get("en", {})).get("value")
    except Exception:
        return None

def resolve_with_optional_wikidata(term: str, use_wd=True, sleep_sec=0.1):
    """Wikipedia→Wikidataの順に解決（ja>en>元のzh）"""
    if not term or str(term).strip() == "":
        return {"ja": term}
    ja = lookup_wikipedia(term)
    if not ja and use_wd:
        ja = lookup_wikidata(term)
    time.sleep(sleep_sec)
    return {"ja": ja or term}

# =============================== Main =======================================

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", required=True, help="入力CSV")
    ap.add_argument("--output", required=True, help="出力CSV")
    ap.add_argument("--brand-col", default="brand")
    ap.add_argument("--model-col", default="model")
    ap.add_argument("--brand-ja-col", default="brand_ja")
    ap.add_argument("--model-ja-col", default="model_ja")
    ap.add_argument("--cache", default=".cache/global_map.json")
    ap.add_argument("--sleep", type=float, default=0.1)
    ap.add_argument("--use-wikidata", action="store_true", help="Wikidata補完を有効化")
    ap.add_argument("--use-official", action="store_true",
                    help="公式サイトCSE補完を有効化（環境変数 GOOGLE_API_KEY / GOOGLE_CSE_ID が必要）")
    args = ap.parse_args()

    print(f"Translating: {args.input} -> {args.output}")

    # 出力/キャッシュディレクトリ
    if os.path.dirname(args.output):
        os.makedirs(os.path.dirname(args.output), exist_ok=True)
    if os.path.dirname(args.cache):
        os.makedirs(os.path.dirname(args.cache), exist_ok=True)

    # キャッシュ
    cache = {}
    if os.path.exists(args.cache):
        with open(args.cache, "r", encoding="utf-8") as f:
            try:
                cache = json.load(f)
            except Exception:
                cache = {}

    # 入力
    df = pd.read_csv(args.input)
    brands = df[args.brand_col].dropna().unique().tolist()
    models = df[args.model_col].dropna().unique().tolist()

    brand_map, model_map = {}, {}

    # ----------------------------- Brand -------------------------------------
    for b in tqdm(brands, desc="brand"):
        key = f"brand::{b}"
        if key in cache:
            brand_map[b] = cache[key]
            continue
        res = resolve_with_optional_wikidata(b, use_wd=args.use_wikidata, sleep_sec=args.sleep)
        brand_map[b] = res["ja"]
        cache[key] = brand_map[b]

    # ----------------------------- Model -------------------------------------
    for m in tqdm(models, desc="model"):
        key = f"model::{m}"
        if key in cache:
            model_map[m] = cache[key]
            continue
        res = resolve_with_optional_wikidata(m, use_wd=args.use_wikidata, sleep_sec=args.sleep)
        ja = res["ja"]

        # 公式サイトCSEフォールバック（モデル単体の先行キャッシュ）
        if args.use_official and (not ja or ja == m):
            try:
                from tools.official_lookup import find_official_english
                guessed = find_official_english("", m)  # ブランド不明の先行推定（後で行単位でも再試行）
                if guessed:
                    ja = guessed
            except Exception:
                pass

        model_map[m] = ja
        cache[key] = ja
        time.sleep(args.sleep)

    # ----------------------------- 書き戻し ----------------------------------
    # ブランド訳
    df[args.brand_ja_col] = df[args.brand_col].map(lambda x: brand_map.get(str(x), str(x)))

    # モデル訳（行ごとにブランド文脈で追加の公式CSEを試す）
    def _model_name(row):
        m = str(row[args.model_col])
        ja = model_map.get(m, m)
        if args.use_official and (not ja or ja == m):
            try:
                from tools.official_lookup import find_official_english
                guessed = find_official_english(str(row[args.brand_col]), m)
                if guessed:
                    return guessed
            except Exception:
                pass
        return ja

    df[args.model_ja_col] = df.apply(_model_name, axis=1)

    # 出力・キャッシュ保存
    df.to_csv(args.output, index=False, encoding="utf-8-sig")
    with open(args.cache, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

    print(f"✅ Done. Saved to {args.output}")

if __name__ == "__main__":
    main()
