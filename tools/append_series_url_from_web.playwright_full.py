#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
append_series_url_from_web.playwright_full.py  (robust)
- /rank/1 ã‚’ Playwright ã§æœ€ä¸‹æ®µã¾ã§ãƒ­ãƒ¼ãƒ‰
- DOMã‹ã‚‰ series_id ã‚’ã€Œå‡ºç¾é †ã€ã§æŠ½å‡ºï¼ˆ=é †ä½ï¼‰
- å…¥åŠ›CSVã« series_url åˆ—ã¨ã—ã¦ä»˜ä¸

ãƒã‚¤ãƒ³ãƒˆ:
- XHR("frontapi/rank/series") ã®å®Œäº†ã‚’â€œãƒ­ãƒ¼ãƒ‰å®Œäº†ã®ç›®å°â€ã¨ã—ã¦å¾…ã¤ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹æœ¬æ–‡ã¯ä½¿ã‚ãªã„ï¼‰
- button[data-series-id] / a[href*="//www.autohome.com.cn/æ•°å­—/"] ä¸¡æ–¹ã§æŠ½å‡º
- ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼‹å¢—åˆ†ç›£è¦–ã§50ä½ã¾ã§è¡¨ç¤º
- 0ä»¶æ™‚ã¯ãƒ‡ãƒãƒƒã‚°HTML/ã‚¹ã‚¯ã‚·ãƒ§ã‚’ data/ ã«ä¿å­˜
"""

import asyncio, re, argparse, pandas as pd, time
from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError as PWTimeout

RANK_XHR_KEYWORD = "rank"  # â€œfrontapi/rank/seriesâ€ ãªã© rank ã‚’å«ã‚€Xhrã‚’å¾…ã¤

async def wait_rank_filled(page, timeout_ms=60000):
    """ãƒ©ãƒ³ã‚­ãƒ³ã‚°XHRãŒè¿”ã‚Šã€DOMã«ã‚«ãƒ¼ãƒ‰ãŒå‡ºã‚‹ã¾ã§å¾…ã¤"""
    # 1) XHRå®Œäº†å¾…ã¡ï¼ˆæœ¬æ–‡ã¯ä½¿ã‚ãªã„ï¼‰
    try:
        await page.wait_for_response(
            lambda r: (r.status == 200) and (RANK_XHR_KEYWORD in r.url),
            timeout=timeout_ms
        )
    except PWTimeout:
        pass  # æ¬¡ã®DOMå¾…ã¡ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    # 2) DOMã«æœ€åˆã®ã‚«ãƒ¼ãƒ‰ãŒç¾ã‚Œã‚‹ã¾ã§å¾…ã¤ï¼ˆã„ãšã‚Œã‹ãŒè¦‹ãˆã‚Œã°OKï¼‰
    sel_any = 'button[data-series-id], a[href*="//www.autohome.com.cn/"]'
    await page.wait_for_selector(sel_any, state="visible", timeout=timeout_ms)

async def robust_scroll_to_bottom(page, rounds=40, idle_ms=500, min_delta=1, max_items=60):
    """ä¸‹ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã€‚æŠ½å‡ºä»¶æ•°ãŒå¢—ãˆãªããªã‚‹ã¾ã§å›ã™"""
    prev = 0
    for i in range(rounds):
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await asyncio.sleep(idle_ms/1000)

        ids = await extract_series_ids_from_dom(page)
        cur = len(ids)
        print(f"  â¤· round {i+1}: {cur}ä»¶ (+{cur-prev})")
        if cur >= max_items:
            return ids[:max_items]
        if (cur - prev) < min_delta and i >= 2:
            # ä¸€åº¦æœ€ä¸Šéƒ¨â†’æœ€ä¸‹éƒ¨ã®â€œæºã•ã¶ã‚Šâ€ã§lazyè¦ç´ ã‚’èµ·ã“ã™
            await page.evaluate("window.scrollTo(0, 0)")
            await asyncio.sleep(0.3)
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(idle_ms/1000)
            # ã‚‚ã†ä¸€åº¦æ•°ãˆã‚‹
            ids2 = await extract_series_ids_from_dom(page)
            if len(ids2) - cur < min_delta:
                return ids2[:max_items]
        prev = cur
    return await extract_series_ids_from_dom(page)

async def extract_series_ids_from_dom(page):
    """DOMã‹ã‚‰ series_id ã‚’å‡ºç¾é †ã§æŠ½å‡ºï¼ˆbutton[data-series-id] ã¨ hrefä¸¡å¯¾å¿œï¼‰"""
    ids = []

    # 1) button[data-series-id]
    try:
        btns = await page.locator('button[data-series-id]').element_handles()
        for h in btns:
            sid = await h.get_attribute("data-series-id")
            if sid and sid.isdigit() and sid not in ids:
                ids.append(sid)
    except Exception:
        pass

    # 2) a[href*="//www.autohome.com.cn/xxxx/"]
    try:
        hrefs = await page.eval_on_selector_all(
            'a[href*="//www.autohome.com.cn/"]',
            "els => els.map(e => e.getAttribute('href'))"
        )
        for href in hrefs or []:
            if not href:
                continue
            m = re.search(r'//www\.autohome\.com\.cn/(\d{3,7})/', href)
            if m:
                sid = m.group(1)
                if sid not in ids:
                    ids.append(sid)
    except Exception:
        pass

    return ids

async def run(rank_url, input_csv, output_csv, name_col, max_rounds, idle_ms, min_delta):
    from playwright.async_api import Error as PWError
    Path("data").mkdir(exist_ok=True, parents=True)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True, args=["--disable-gpu"])
        page = await browser.new_page()
        print(f"ğŸŒ é–‹å§‹: {rank_url}")
        await page.goto(rank_url, wait_until="networkidle")

        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒæ³¨å…¥ã•ã‚Œã‚‹ã¾ã§å¾…ã¤
        await wait_rank_filled(page, timeout_ms=60000)

        # ã—ã£ã‹ã‚Šä¸‹ã¾ã§è¡¨ç¤ºã•ã›ã‚‹
        ids = await robust_scroll_to_bottom(
            page, rounds=max_rounds, idle_ms=idle_ms, min_delta=min_delta, max_items=60
        )

        if not ids:
            # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            Path("data/_debug_rank_page.html").write_text(await page.content(), encoding="utf-8")
            await page.screenshot(path="data/_debug_rank_page.png", full_page=True)
            print("âš ï¸ 0ä»¶ã§ã—ãŸã€‚data/_debug_rank_page.html / .png ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        await browser.close()

    # å…¥å‡ºåŠ›
    df = pd.read_csv(input_csv)
    n = min(len(df), len(ids))
    urls = [f"https://www.autohome.com.cn/{sid}/" for sid in ids[:n]]
    out = df.head(n).copy()
    out["series_url"] = urls
    out.to_csv(output_csv, index=False, encoding="utf-8-sig")
    print(f"âœ… æŠ½å‡ºå®Œäº†: {len(ids)}ä»¶ / ä¿å­˜: {output_csv}ï¼ˆ{n}è¡Œã«ä»˜ä¸ï¼‰")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--rank-url", default="https://www.autohome.com.cn/rank/1")
    ap.add_argument("--input", required=True)
    ap.add_argument("--output", required=True)
    ap.add_argument("--name-col", default="model")
    ap.add_argument("--max-rounds", type=int, default=40)
    ap.add_argument("--idle-ms", type=int, default=600)
    ap.add_argument("--min-delta", type=int, default=1)
    args = ap.parse_args()
    asyncio.run(run(args.rank_url, args.input, args.output, args.name_col,
                    args.max_rounds, args.idle_ms, args.min_delta))

if __name__ == "__main__":
    main()
