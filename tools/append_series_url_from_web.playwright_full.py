#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Autohomeã® /rank/ ãƒšãƒ¼ã‚¸ã‚’ Playwright ã§é–‹ãã€è»Šç¨®ãƒšãƒ¼ã‚¸(series_url)ã‚’æŠ½å‡ºã—ã¦
å…¥åŠ›CSVã« left-join ã—ã€series_url ã‚«ãƒ©ãƒ ã‚’ä»˜ä¸ã™ã‚‹ã€‚

å‰æ:
- å…¥åŠ›CSVã«ã¯å°‘ãªãã¨ã‚‚ rank(æ•´æ•°) ãŒã‚ã‚‹ã“ã¨
- å‡ºåŠ›CSVã« series_url ã‚’è¿½è¨˜
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ä½¿ã‚ãªã„(æ¯å›å–å¾—)

ä½¿ã„æ–¹:
python tools/append_series_url_from_web.playwright_full.py \
  --rank-url https://www.autohome.com.cn/rank/1-3-1071-x/ \
  --input data/autohome_raw_2025-08.csv \
  --output data/autohome_raw_2025-08_with_series.csv \
  --name-col model --max-rounds 1 --idle-ms 200 --min-delta 0
"""

import argparse
import asyncio
import csv
import re
from typing import Dict, List, Tuple

import pandas as pd
from playwright.async_api import async_playwright

RANK_ROW_RE = re.compile(r'/(\d+)/')  # e.g. href="https://www.autohome.com.cn/7806/"

def _parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--rank-url", required=True)
    p.add_argument("--input", required=True)
    p.add_argument("--output", required=True)
    p.add_argument("--name-col", default="model")  # CSVå´ã®ãƒ¢ãƒ‡ãƒ«ååˆ—(ç…§åˆã®å‚è€ƒã«ä½¿ã†ã ã‘)
    p.add_argument("--max-rounds", type=int, default=1)
    p.add_argument("--idle-ms", type=int, default=200)
    p.add_argument("--min-delta", type=int, default=0)
    return p.parse_args()

async def fetch_rank_table(rank_url: str) -> List[Tuple[int, str]]:
    """
    /rank/ãƒšãƒ¼ã‚¸ã‚’é–‹ã„ã¦ (rank, series_url) ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    rank ã¯ 1 å§‹ã¾ã‚Šã€‚series_url ã¯ https://www.autohome.com.cn/<id>/ ã®å®Œå…¨URLã«æƒãˆã‚‹
    """
    results: List[Tuple[int, str]] = []
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(rank_url, wait_until="networkidle", timeout=60000)

        # Autohomeã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨è¡Œã‚’å…¨éƒ¨æ‹¾ã†ã€‚è¡Œã”ã¨ã« rank ã¨ a[href] ã‚’å–å¾—
        rows = await page.query_selector_all("table, .rank-list, .rank-list table, .rank-table tr, tr")
        # ä¸Šè¨˜ã¯ä¿é™ºã€‚å®Ÿä½“ã¯â€¦ãƒªãƒ³ã‚¯ãŒ /<series_id>/ ã‚’å«ã‚€ a ã‚’åˆ—æŒ™ã—ã€é †ç•ª=rank ã¨ã¿ãªã™
        anchors = await page.query_selector_all("a[href*='autohome.com.cn/']")
        hrefs = []
        for a in anchors:
            href = await a.get_attribute("href")
            if not href:
                continue
            m = RANK_ROW_RE.search(href)
            if m:
                # æ­£è¦åŒ–(ãƒ—ãƒ­ãƒˆã‚³ãƒ«/æœ«å°¾ã‚¹ãƒ©ä»˜ä¸)
                sid = m.group(1)
                hrefs.append(f"https://www.autohome.com.cn/{sid}/")

        # å‡ºç¾é †ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼ˆåŒã˜seriesã¸ã®è¤‡æ•°ãƒªãƒ³ã‚¯ãŒã‚ã‚‹ãŸã‚ï¼‰
        seen = set()
        uniq = []
        for h in hrefs:
            if h not in seen:
                uniq.append(h)
                seen.add(h)

        for idx, h in enumerate(uniq, start=1):
            results.append((idx, h))

        await browser.close()
    return results

def left_join_series_url(df: pd.DataFrame, rank_map: Dict[int, str]) -> pd.DataFrame:
    df2 = df.copy()
    if "rank" not in df2.columns:
        # rank_seq ã—ã‹ç„¡ã„å ´åˆã®ä¿é™º
        if "rank_seq" in df2.columns:
            df2["rank"] = df2["rank_seq"]
        else:
            raise KeyError("input CSV must have 'rank' or 'rank_seq' column.")

    df2["rank"] = pd.to_numeric(df2["rank"], errors="coerce").astype("Int64")
    df2["series_url"] = df2["rank"].map(rank_map)
    return df2

def main():
    args = _parse_args()
    print(f"ğŸ§¾ input: {args.input}")
    print(f"ğŸŒ scraping: {args.rank_url}")

    rank_pairs = asyncio.run(fetch_rank_table(args.rank_url))
    rank_map = {r: u for r, u in rank_pairs}
    if not rank_map:
        print("â–² No series urls found in HTML")
    else:
        print(f"âœ“ scraped {len(rank_map)} series urls")

    df = pd.read_csv(args.input)
    out = left_join_series_url(df, rank_map)
    out.to_csv(args.output, index=False, quoting=csv.QUOTE_MINIMAL)
    print(f"â–º {args.input} -> {args.output}")

if __name__ == "__main__":
    main()
