#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
append_series_url_from_web.playwright_full.py  (robust, event-wait compatible)
- /rank/1 „Çí Playwright „ÅßÊúÄ‰∏ãÊÆµ„Åæ„Åß„É≠„Éº„Éâ
- DOM„Åã„Çâ series_id „Çí„ÄåÂá∫ÁèæÈ†Ü„Äç„ÅßÊäΩÂá∫Ôºà=È†Ü‰ΩçÔºâ
- ÂÖ•ÂäõCSV„Å´ series_url Âàó„Å®„Åó„Å¶‰ªò‰∏é
"""

import asyncio, re, argparse, pandas as pd
from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError as PWTimeout

RANK_XHR_KEYWORD = "rank"  # ‚Äúfrontapi/rank/series‚Äù „Å™„Å© 'rank' „ÇíÂê´„ÇÄXHR„ÇíÂæÖ„Å§

async def _wait_any_response_with_keyword(page, keyword: str, timeout_ms: int = 60000):
    """
    page.wait_for_response „ÅåÁÑ°„ÅÑÁí∞Â¢É„Åß„ÇÇÂãï„Åè„Çà„ÅÜ„Å´„ÄÅwait_for_event('response', ...) „Çí‰Ωø„Å£„Å¶ÂæÖ„Å§„ÄÇ
    """
    try:
        await page.wait_for_event(
            "response",
            predicate=lambda r: (r is not None) and (r.status == 200) and (keyword in r.url),
            timeout=timeout_ms
        )
    except PWTimeout:
        # XHR„ÅåË¶ã„Å§„Åã„Çâ„Å™„Åè„Å¶„ÇÇÂæåÊÆµ„ÅÆDOMÂæÖ„Å°„Å∏„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
        pass

async def wait_rank_filled(page, timeout_ms=60000):
    """
    „É©„É≥„Ç≠„É≥„Ç∞XHR„ÅåËøî„Çä„ÄÅDOM„Å´„Ç´„Éº„Éâ„ÅåÂá∫„Çã„Åæ„ÅßÂæÖ„Å§
    """
    # 1) XHRÂÆå‰∫ÜÂæÖ„Å°ÔºàÊú¨Êñá„ÅØ‰Ωø„Çè„Å™„ÅÑÔºâ
    await _wait_any_response_with_keyword(page, RANK_XHR_KEYWORD, timeout_ms=timeout_ms)

    # 2) DOM„Å´ÊúÄÂàù„ÅÆ„Ç´„Éº„Éâ„ÅåÁèæ„Çå„Çã„Åæ„ÅßÂæÖ„Å§Ôºà„ÅÑ„Åö„Çå„Åã„ÅåË¶ã„Åà„Çå„Å∞OKÔºâ
    sel_any = 'button[data-series-id], a[href*="//www.autohome.com.cn/"], [data-rank-num], div.rank-num'
    await page.wait_for_selector(sel_any, state="visible", timeout=timeout_ms)

async def extract_series_ids_from_dom(page):
    """
    DOM„Åã„Çâ series_id „ÇíÂá∫ÁèæÈ†Ü„ÅßÊäΩÂá∫Ôºàbutton[data-series-id] „Å® href‰∏°ÂØæÂøúÔºâ
    """
    ids = []

    # 1) button[data-series-id]
    try:
        btns = await page.locator('button[data-series-id]').element_handles()
        for h in btns:
            sid = await h.get_attribute("data-series-id")
            if sid and sid.isdigit() and sid not in ids:
                ids.append(sid)
    except Exception:
        pass

    # 2) a[href*="//www.autohome.com.cn/xxxx/"]
    try:
        hrefs = await page.eval_on_selector_all(
            'a[href*="//www.autohome.com.cn/"]',
            "els => els.map(e => e.getAttribute('href'))"
        )
        for href in hrefs or []:
            if not href:
                continue
            m = re.search(r'//www\.autohome\.com\.cn/(\d{3,7})/', href)
            if m:
                sid = m.group(1)
                if sid not in ids:
                    ids.append(sid)
    except Exception:
        pass

    return ids

async def robust_scroll_to_bottom(page, rounds=40, idle_ms=600, min_delta=1, max_items=60):
    """
    ‰∏ã„Åæ„Åß„Çπ„ÇØ„É≠„Éº„É´„ÄÇÊäΩÂá∫‰ª∂Êï∞„ÅåÂ¢ó„Åà„Å™„Åè„Å™„Çã„Åæ„ÅßÂõû„Åô
    """
    prev = 0
    for i in range(rounds):
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await asyncio.sleep(idle_ms/1000)

        ids = await extract_series_ids_from_dom(page)
        cur = len(ids)
        print(f"  ‚§∑ round {i+1}: {cur}‰ª∂ (+{cur-prev})")
        if cur >= max_items:
            return ids[:max_items]
        if (cur - prev) < min_delta and i >= 2:
            # Êè∫„Åï„Å∂„Çä
            await page.evaluate("window.scrollTo(0, 0)")
            await asyncio.sleep(0.3)
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(idle_ms/1000)
            ids2 = await extract_series_ids_from_dom(page)
            if len(ids2) - cur < min_delta:
                return ids2[:max_items]
        prev = cur
    return await extract_series_ids_from_dom(page)

async def run(rank_url, input_csv, output_csv, name_col, max_rounds, idle_ms, min_delta):
    Path("data").mkdir(exist_ok=True, parents=True)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True, args=["--disable-gpu"])
        page = await browser.new_page()
        print(f"üåê ÈñãÂßã: {rank_url}")
        await page.goto(rank_url, wait_until="networkidle")

        # „É©„É≥„Ç≠„É≥„Ç∞„ÅåÊ≥®ÂÖ•„Åï„Çå„Çã„Åæ„ÅßÂæÖ„Å§Ôºà‰∫íÊèõÁâàÔºâ
        await wait_rank_filled(page, timeout_ms=60000)

        # „Åó„Å£„Åã„Çä‰∏ã„Åæ„ÅßË°®Á§∫„Åï„Åõ„Çã
        ids = await robust_scroll_to_bottom(
            page, rounds=max_rounds, idle_ms=idle_ms, min_delta=min_delta, max_items=60
        )

        if not ids:
            # „Éá„Éê„ÉÉ„Ç∞Âá∫Âäõ
            Path("data/_debug_rank_page.html").write_text(await page.content(), encoding="utf-8")
            await page.screenshot(path="data/_debug_rank_page.png", full_page=True)
            print("‚ö†Ô∏è 0‰ª∂„Åß„Åó„Åü„ÄÇdata/_debug_rank_page.html / .png „ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")

        await browser.close()

    # ÂÖ•Âá∫Âäõ
    df = pd.read_csv(input_csv)
    n = min(len(df), len(ids))
    urls = [f"https://www.autohome.com.cn/{sid}/" for sid in ids[:n]]
    out = df.head(n).copy()
    out["series_url"] = urls
    out.to_csv(output_csv, index=False, encoding="utf-8-sig")
    print(f"‚úÖ ÊäΩÂá∫ÂÆå‰∫Ü: {len(ids)}‰ª∂ / ‰øùÂ≠ò: {output_csv}Ôºà{n}Ë°å„Å´‰ªò‰∏éÔºâ")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--rank-url", default="https://www.autohome.com.cn/rank/1")
    ap.add_argument("--input", required=True)
    ap.add_argument("--output", required=True)
    ap.add_argument("--name-col", default="model")
    ap.add_argument("--max-rounds", type=int, default=40)
    ap.add_argument("--idle-ms", type=int, default=600)
    ap.add_argument("--min-delta", type=int, default=1)
    args = ap.parse_args()
    asyncio.run(run(args.rank_url, args.input, args.output, args.name_col,
                    args.max_rounds, args.idle_ms, args.min_delta))

if __name__ == "__main__":
    main()
