from __future__ import annotations
import os, json, time, re
from pathlib import Path
import pandas as pd
from openai import OpenAI

# ====== å…¥å‡ºåŠ›ï¼ˆYAMLå¤‰æ›´ãªã—ã§å‹•ãã‚ˆã†äº’æ›é‡è¦–ï¼‰ ======
SERIES_ID = os.environ.get("SERIES_ID", "").strip()

def resolve_src_dst():
    csv_in  = os.environ.get("CSV_IN", "").strip()
    csv_out = os.environ.get("CSV_OUT", "").strip()

    def guess_paths_from_series(sid: str):
        if not sid:
            return None, None
        base = f"output/autohome/{sid}/config_{sid}"
        return Path(f"{base}.csv"), Path(f"{base}.ja.csv")  # æ—¢å®šã¯ .ja.csv

    default_in  = Path("output/autohome/7578/config_7578.csv")
    default_out = Path("output/autohome/7578/config_7578.ja.csv")

    src = Path(csv_in)  if csv_in  else None
    dst = Path(csv_out) if csv_out else None

    if src is None or dst is None:
        s2, d2 = guess_paths_from_series(SERIES_ID)
        src = src or s2
        dst = dst or d2

    src = src or default_in
    dst = dst or default_out
    return src, dst

SRC, DST_PRIMARY = resolve_src_dst()

def make_secondary(dst: Path) -> Path:
    s = dst.name
    if s.endswith(".ja.csv"):
        s2 = s.replace(".ja.csv", "_ja.csv")
    elif s.endswith("_ja.csv"):
        s2 = s.replace("_ja.csv", ".ja.csv")
    else:
        s2 = dst.stem + ".ja.csv"
    return dst.parent / s2

DST_SECONDARY = make_secondary(DST_PRIMARY)

# ====== OpenAI ======
MODEL   = os.environ.get("OPENAI_MODEL", "gpt-4.1-mini")
API_KEY = os.environ.get("OPENAI_API_KEY")

# ã‚¹ã‚¤ãƒƒãƒ
TRANSLATE_VALUES   = os.environ.get("TRANSLATE_VALUES", "true").lower() == "true"
TRANSLATE_COLNAMES = os.environ.get("TRANSLATE_COLNAMES", "true").lower() == "true"

# å…ˆé ­è»Šåã‚’å‰Šã‚‹ï¼ˆæ—¢å®šONï¼‰ã€‚æ˜ç¤ºãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ SERIES_PREFIXï¼ˆä¾‹: "é§†é€è‰¦05|é©±é€èˆ°05"ï¼‰
STRIP_GRADE_PREFIX = os.environ.get("STRIP_GRADE_PREFIX", "true").lower() == "true"
SERIES_PREFIX_RE   = os.environ.get("SERIES_PREFIX", "").strip()

# CNYâ†’JPY
EXRATE_CNY_TO_JPY  = float(os.environ.get("EXRATE_CNY_TO_JPY", "21.0"))

BATCH_SIZE  = 60
RETRIES     = 3
SLEEP_BASE  = 1.2

# ====== ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»è¾æ›¸ ======
NOISE_ANY = ["å¯¹æ¯”", "å‚æ•°", "å›¾ç‰‡", "é…ç½®", "è¯¦æƒ…"]
NOISE_PRICE_TAIL = ["è¯¢ä»·", "è®¡ç®—å™¨", "è¯¢åº•ä»·", "æŠ¥ä»·", "ä»·æ ¼è¯¢é—®", "ä¾¡æ ¼å•ã„åˆã‚ã›", "èµ·", "èµ·å”®"]

def clean_any_noise(s: str) -> str:
    s = str(s) if s is not None else ""
    for w in NOISE_ANY + NOISE_PRICE_TAIL:
        s = s.replace(w, "")
    s = re.sub(r"\s+", " ", s).strip(" ã€€-â€”â€“")
    return s

def clean_price_cell(s: str) -> str:
    t = clean_any_noise(s)
    for w in NOISE_PRICE_TAIL:
        t = re.sub(rf"(?:\s*{re.escape(w)}\s*)+$", "", t)
    return t.strip()

# ãƒ–ãƒ©ãƒ³ãƒ‰æ­£è¦åŒ–ï¼ˆBYDã¯ç¿»è¨³ã—ãªã„ï¼‰
BRAND_MAP = {"BYD": "BYD", "æ¯”äºšè¿ª": "BYD"}

# å›ºå®šè¨³
# ãƒ»ãƒ¡ãƒ¼ã‚«ãƒ¼å¸Œæœ›å°å£²ä¾¡æ ¼ï¼šé€šè²¨è¡¨è¨˜ãªã—
# ãƒ»ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼ä¾¡æ ¼ï¼šè¦‹å‡ºã—ã«ï¼ˆå…ƒï¼‰ã‚’æ˜è¨˜
FIX_JA_ITEMS = {
    "å‚å•†æŒ‡å¯¼ä»·":   "ãƒ¡ãƒ¼ã‚«ãƒ¼å¸Œæœ›å°å£²ä¾¡æ ¼",      # â˜…ï¼ˆå††ï¼‰/ï¼ˆå…ƒï¼‰ãªã©ä»˜ã‘ãªã„
    "ç»é”€å•†å‚è€ƒä»·": "ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼è²©å£²ä¾¡æ ¼ï¼ˆå…ƒï¼‰",
    "ç»é”€å•†æŠ¥ä»·":   "ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼è²©å£²ä¾¡æ ¼ï¼ˆå…ƒï¼‰",
    "ç»é”€å•†":       "ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼è²©å£²ä¾¡æ ¼ï¼ˆå…ƒï¼‰",
    "è¢«åŠ¨å®‰å…¨":     "è¡çªå®‰å…¨",
}
FIX_JA_SECTIONS = {"è¢«åŠ¨å®‰å…¨": "è¡çªå®‰å…¨"}

PRICE_ITEM_MSRP_CN = {"å‚å•†æŒ‡å¯¼ä»·"}
PRICE_ITEM_MSRP_JA = {"ãƒ¡ãƒ¼ã‚«ãƒ¼å¸Œæœ›å°å£²ä¾¡æ ¼"}
PRICE_ITEM_DEALER_CN = {"ç»é”€å•†å‚è€ƒä»·", "ç»é”€å•†æŠ¥ä»·", "ç»é”€å•†"}
PRICE_ITEM_DEALER_JA = {"ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼è²©å£²ä¾¡æ ¼ï¼ˆå…ƒï¼‰"}

# ====== ä¾¡æ ¼æ•´å½¢ ======
RE_WAN       = re.compile(r"(?P<num>\d+(?:\.\d+)?)\s*ä¸‡")
RE_YUAN      = re.compile(r"(?P<num>[\d,]+)\s*å…ƒ")
RE_JPY_PAREN = re.compile(r"ï¼ˆæ—¥æœ¬å††[0-9,]+å††ï¼‰|ï¼ˆç´„Â¥[0-9,]+ï¼‰")

def parse_cny(text: str):
    """æ–‡å­—åˆ—ã‹ã‚‰ CNY é‡‘é¡ï¼ˆå…ƒï¼‰ã‚’æŠ½å‡ºã€‚ä¸‡â†’å…ƒ ã«æ›ç®—ã€‚å¤±æ•—æ™‚ Noneã€‚"""
    m1 = RE_WAN.search(text)
    if m1:
        return float(m1.group("num")) * 10000.0
    m2 = RE_YUAN.search(text)
    if m2:
        return float(m2.group("num").replace(",", ""))
    return None

def msrp_to_yuan_and_jpy(cell: str, rate: float) -> str:
    """
    ãƒ¡ãƒ¼ã‚«ãƒ¼å¸Œæœ›å°å£²ä¾¡æ ¼ã®ã‚»ãƒ«ã‚’ã€Œxxä¸‡å…ƒï¼ˆæ—¥æœ¬å††YYYå††ï¼‰ã€ã«çµ±ä¸€ã€‚
    ãƒ»æ—¢å­˜ã®å††è¡¨è¨˜ã¯ä¸€æ—¦é™¤å»ã—ã¦å†ç”Ÿæˆ
    ãƒ»ã€Œ11.98ä¸‡ã€â†’ã€Œ11.98ä¸‡å…ƒï¼ˆæ—¥æœ¬å††251,580å††ï¼‰ã€ã®ã‚ˆã†ã«å‡ºåŠ›
    ãƒ»ãƒ€ãƒƒã‚·ãƒ¥ç­‰ã¯ãã®ã¾ã¾
    """
    t = str(cell).strip()
    if not t or t in {"-", "â€“", "â€”"}:
        return t
    t = RE_JPY_PAREN.sub("", t).strip()

    cny = parse_cny(t)
    if cny is None:
        # æœ«å°¾ã«ã€Œå…ƒã€ã‚’ä»˜ä¸ã ã‘ï¼ˆæƒ…å ±ãŒç„¡ã„å ´åˆã¯è§¦ã‚‰ãªã„ï¼‰
        if ("å…ƒ" not in t) and RE_WAN.search(t):
            t = f"{t}å…ƒ"
        return t

    # è¡¨ç¤ºç”¨ï¼šå…ƒã¯ã€Œä¸‡ã€è¡¨è¨˜ã‚’å°Šé‡ï¼ˆ"11.98ä¸‡" ãŒæ®‹ã£ã¦ã„ã‚Œã°ãã‚Œã‚’ãƒ™ãƒ¼ã‚¹ã«ï¼‰
    m1 = RE_WAN.search(t)
    if m1:
        yuan_disp = f"{m1.group('num')}ä¸‡å…ƒ"
    else:
        # 129,800å…ƒ â†’ ã€Œ129,800å…ƒã€
        if "å…ƒ" not in t:
            t = f"{t}å…ƒ"
        yuan_disp = t

    jpy = int(round(cny * rate))
    jpy_fmt = f"{jpy:,}"
    return f"{yuan_disp}ï¼ˆæ—¥æœ¬å††{jpy_fmt}å††ï¼‰"

def dealer_to_yuan_only(cell: str) -> str:
    """
    ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼ä¾¡æ ¼ã¯ã€Œâ€¦å…ƒã€ã ã‘ï¼ˆå††ã¯ä»˜ã‘ãªã„ï¼‰ã€‚
    ãƒ»æ—¢å­˜ã®å††è¡¨è¨˜ã¯é™¤å»
    ãƒ»ã€Œ11.98ä¸‡ã€ã«ã¯ã€Œå…ƒã€ã‚’æ˜è¨˜ã—ã¦ã€Œ11.98ä¸‡å…ƒã€
    """
    t = str(cell).strip()
    if not t or t in {"-", "â€“", "â€”"}:
        return t
    t = RE_JPY_PAREN.sub("", t).strip()
    if ("å…ƒ" not in t) and RE_WAN.search(t):
        t = f"{t}å…ƒ"
    return t

# ====== LLM ======
def uniq(seq):
    s, out = set(), []
    for x in seq:
        if x not in s:
            s.add(x); out.append(x)
    return out

def chunked(xs, n):
    for i in range(0, len(xs), n):
        yield xs[i:i+n]

def parse_json_relaxed(content: str, terms: list[str]) -> dict[str, str]:
    try:
        data = json.loads(content)
        if isinstance(data, dict) and "translations" in data:
            m = {}
            for d in data["translations"]:
                cn = str(d.get("cn", "")).strip()
                ja = str(d.get("ja", "")).strip()
                if cn:
                    m[cn] = ja or cn
            if m:
                return m
    except Exception:
        pass
    mjson = re.search(r"\{[\s\S]*\}", content)
    if mjson:
        try:
            data = json.loads(mjson.group(0))
            if isinstance(data, dict) and "translations" in data:
                m = {}
                for d in data["translations"]:
                    cn = str(d.get("cn", "")).strip()
                    ja = str(d.get("ja", "")).strip()
                    if cn:
                        m[cn] = ja or cn
                if m:
                    return m
        except Exception:
            pass
    m = {}
    for line in content.splitlines():
        if "\t" in line:
            cn, ja = line.split("\t", 1)
            cn = cn.strip(); ja = ja.strip()
            if cn:
                m[cn] = ja or cn
    for t in terms:
        m.setdefault(t, t)
    return m

class Translator:
    def __init__(self, model: str, api_key: str):
        if not api_key:
            raise RuntimeError("OPENAI_API_KEY is not set")
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.system = (
            "ã‚ãªãŸã¯è‡ªå‹•è»Šä»•æ§˜è¡¨ã®å°‚é–€ç¿»è¨³è€…ã§ã™ã€‚"
            "å…¥åŠ›ã¯ä¸­å›½èªã®ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³å/é …ç›®å/ãƒ¢ãƒ‡ãƒ«å/ã‚»ãƒ«å€¤ã€ã®é…åˆ—ã§ã™ã€‚"
            "è‡ªç„¶ã§ç°¡æ½”ãªæ—¥æœ¬èªã¸ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚æ•°å€¤ãƒ»å¹´å¼ãƒ»æ’é‡ãƒ»AT/MTç­‰ã®è¨˜å·ã¯ä¿æŒã€‚"
            "å‡ºåŠ›ã¯ JSONï¼ˆ{'translations':[{'cn':'åŸæ–‡','ja':'è¨³æ–‡'}]}ï¼‰ã®ã¿ã€‚"
        )

    def translate_batch(self, terms: list[str]) -> dict[str, str]:
        msgs = [
            {"role": "system", "content": self.system},
            {"role": "user", "content": json.dumps({"terms": terms}, ensure_ascii=False)},
        ]
        resp = self.client.chat.completions.create(
            model=self.model,
            messages=msgs,
            temperature=0,
            response_format={"type": "json_object"},
        )
        content = resp.choices[0].message.content or ""
        return parse_json_relaxed(content, terms)

    def translate_unique(self, unique_terms: list[str]) -> dict[str, str]:
        out = {}
        for chunk in chunked(unique_terms, BATCH_SIZE):
            for attempt in range(1, RETRIES+1):
                try:
                    out.update(self.translate_batch(chunk))
                    break
                except Exception:
                    if attempt == RETRIES:
                        for t in chunk:
                            out.setdefault(t, t)
                    time.sleep(SLEEP_BASE * attempt)
        return out

# ====== å…ˆé ­è»Šåã®ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹å‰Šé™¤ï¼ˆæ±ç”¨åŒ–ï¼‰ ======
# ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼š
#  1) å¹´å¼/ã€Œæ¬¾ã€ãªã©ã®ç›´å‰ã¾ã§ã‚’ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹å€™è£œã¨ã—ã¦æŠ½å‡º
#  2) åˆ—å…¨ä½“ã§é »å‡ºã™ã‚‹å…ˆé ­èªã‚’å…±é€šæ¥é ­è¾ã¨ã¿ãªã—å‰Šé™¤
#  3) æ˜ç¤ºæŒ‡å®šï¼ˆSERIES_PREFIXï¼‰ãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆ
YEAR_TOKEN_RE = re.compile(r"(?:20\d{2}|19\d{2})|(?:\d{2}æ¬¾|[ä¸Šä¸­ä¸‹]å¸‚|æ”¹æ¬¾|å¹´æ¬¾)")
LEADING_TOKEN_RE = re.compile(r"^[\u4e00-\u9fffA-Za-z][\u4e00-\u9fffA-Za-z0-9\- ]{1,40}")

def cut_before_year_or_kuan(s: str) -> str | None:
    s = s.strip()
    m = YEAR_TOKEN_RE.search(s)
    if m:
        return s[:m.start()].strip()
    kuan = re.search(r"æ¬¾", s)
    if kuan:
        return s[:kuan.start()].strip()
    m2 = LEADING_TOKEN_RE.match(s)
    return m2.group(0).strip() if m2 else None

def detect_common_series_prefix(cols: list[str]) -> str | None:
    cand = []
    for c in cols:
        p = cut_before_year_or_kuan(str(c))
        if p and len(p) >= 2:
            cand.append(p)
    if not cand:
        return None
    from collections import Counter
    top, ct = Counter(cand).most_common(1)[0]
    if ct >= max(1, int(0.6 * len(cols))):
        return re.escape(top)
    return None

def strip_series_prefix_from_grades(grade_cols: list[str]) -> list[str]:
    if not grade_cols or not STRIP_GRADE_PREFIX:
        return grade_cols
    pattern = SERIES_PREFIX_RE if SERIES_PREFIX_RE else detect_common_series_prefix(grade_cols)
    if not pattern:
        return grade_cols
    regex = re.compile(rf"^\s*(?:{pattern})\s*[-:ï¼š/ ]*\s*", re.IGNORECASE)
    cleaned = [regex.sub("", str(c)).strip() or c for c in grade_cols]
    return cleaned

# ====== main ======
def main():
    print(f"ğŸ” SRC: {SRC}")
    print(f"ğŸ“ DST(primary): {DST_PRIMARY}")
    print(f"ğŸ“ DST(secondary): {DST_SECONDARY}")

    if not Path(SRC).exists():
        print("âš  å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è¿‘å‚ã®CSVã‚’æ¢ç´¢ã—ã¾ã™â€¦")
        for p in Path("output").glob("**/config_*.csv"):
            print("  -", p)
        raise FileNotFoundError(f"å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {SRC}")

    df = pd.read_csv(SRC, encoding="utf-8-sig")
    df = df.map(clean_any_noise)

    # åˆ—ãƒ˜ãƒƒãƒ€ã®ãƒ–ãƒ©ãƒ³ãƒ‰æ­£è¦åŒ–
    df.columns = [BRAND_MAP.get(c, c) for c in df.columns]

    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³/é …ç›® ç¿»è¨³
    uniq_sec  = uniq([str(x).strip() for x in df["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"].fillna("").tolist() if str(x).strip()])
    uniq_item = uniq([str(x).strip() for x in df["é …ç›®"].fillna("").tolist() if str(x).strip()])

    tr = Translator(MODEL, API_KEY)
    sec_map  = tr.translate_unique(uniq_sec)
    item_map = tr.translate_unique(uniq_item)
    sec_map.update(FIX_JA_SECTIONS)
    item_map.update(FIX_JA_ITEMS)

    out = df.copy()
    out.insert(1, "ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja", out["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"].map(lambda s: sec_map.get(str(s).strip(), str(s).strip())))
    out.insert(3, "é …ç›®_ja",     out["é …ç›®"].map(lambda s: item_map.get(str(s).strip(), str(s).strip())))

    # åˆ—ãƒ˜ãƒƒãƒ€ï¼ˆã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰ç¿»è¨³ï¼†å…ˆé ­è»Šåå‰Šé™¤ï¼ˆæ±ç”¨åŒ–ï¼‰
    if TRANSLATE_COLNAMES:
        orig_cols   = list(out.columns)
        fixed_cols  = orig_cols[:4]
        grade_cols  = orig_cols[4:]
        grade_cols_norm     = [BRAND_MAP.get(c, c) for c in grade_cols]
        grade_cols_stripped = strip_series_prefix_from_grades(grade_cols_norm)
        uniq_grades = uniq([str(c).strip() for c in grade_cols_stripped])
        grade_map   = tr.translate_unique(uniq_grades)
        translated  = [grade_map.get(g, g) or g for g in grade_cols_stripped]
        out.columns = fixed_cols + translated
    else:
        if STRIP_GRADE_PREFIX:
            orig_cols   = list(out.columns)
            fixed_cols  = orig_cols[:4]
            grade_cols  = orig_cols[4:]
            out.columns = fixed_cols + strip_series_prefix_from_grades(grade_cols)

    # ===== ä¾¡æ ¼ã‚»ãƒ«æ•´å½¢ =====
    is_msrp_row   = out["é …ç›®"].isin(list(PRICE_ITEM_MSRP_CN)) | out["é …ç›®_ja"].isin(list(PRICE_ITEM_MSRP_JA))
    is_dealer_row = out["é …ç›®"].isin(list(PRICE_ITEM_DEALER_CN)) | out["é …ç›®_ja"].isin(list(PRICE_ITEM_DEALER_JA))

    # MSRP: ã€Œxxä¸‡å…ƒï¼ˆæ—¥æœ¬å††YYYå††ï¼‰ã€ã€Dealer: ã€Œâ€¦å…ƒã€ã®ã¿
    for col in out.columns[4:]:
        out.loc[is_msrp_row, col] = out.loc[is_msrp_row, col].map(
            lambda s: msrp_to_yuan_and_jpy(clean_price_cell(s), EXRATE_CNY_TO_JPY)
        )
        out.loc[is_dealer_row, col] = out.loc[is_dealer_row, col].map(
            lambda s: dealer_to_yuan_only(clean_price_cell(s))
        )

    # å€¤ã‚»ãƒ«ã®ç¿»è¨³ï¼ˆä¾¡æ ¼è¡Œã¯å¯¾è±¡å¤–ï¼‰
    if TRANSLATE_VALUES:
        values = []
        numeric_like = re.compile(r"^[\d\.\,\%\:/xX\+\-\(\)~ï½\smmkKwWhHVVAhLä¸¨Â·â€”â€“]+$")
        non_price_mask = ~(is_msrp_row | is_dealer_row)
        for col in out.columns[4:]:
            for v in out.loc[non_price_mask, col].astype(str).tolist():
                vv = v.strip()
                if vv in {"", "â—", "â—‹", "â€“", "-", "â€”"}:
                    continue
                if numeric_like.fullmatch(vv):
                    continue
                values.append(vv)
        uniq_vals = uniq(values)
        val_map = tr.translate_unique(uniq_vals)
        for col in out.columns[4:]:
            out.loc[non_price_mask, col] = out.loc[non_price_mask, col].map(
                lambda s: val_map.get(str(s).strip(), str(s).strip())
            )

    # å‡ºåŠ›ï¼ˆArtifacts æºã‚Œå¯¾ç­–ã§äºŒé‡æ›¸ãï¼‰
    DST_PRIMARY.parent.mkdir(parents=True, exist_ok=True)
    out.to_csv(DST_PRIMARY, index=False, encoding="utf-8-sig")
    out.to_csv(DST_SECONDARY, index=False, encoding="utf-8-sig")

    print(f"âœ… Saved: {DST_PRIMARY.resolve()}")
    print(f"âœ… Saved: {DST_SECONDARY.resolve()}")
    print(f"ğŸ“¦ Exists (primary)? {DST_PRIMARY.exists()}")
    print(f"ğŸ“¦ Exists (secondary)? {DST_SECONDARY.exists()}")

if __name__ == "__main__":
    main()
