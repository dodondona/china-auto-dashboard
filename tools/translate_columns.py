import os
import re
import time
from pathlib import Path
import pandas as pd
from openai import OpenAI
import json

# ===== è¨­å®š =====
CACHE_DIR = Path("cache_repo/series")         # ãƒªãƒã‚¸ãƒˆãƒªå†…ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆã‚·ãƒªãƒ¼ã‚ºã”ã¨ã« CN/JA ã‚’ä¸¸ã”ã¨ä¿å­˜ï¼‰
OUTPUT_DIR = Path("output/autohome")          # å‡ºåŠ›å…ˆ
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")

RETRIES = 3
SLEEP_BASE = 1.2
BATCH_SIZE = 60
EXRATE = float(os.environ.get("EXRATE_CNY_TO_JPY", "21.0"))

# ===== åˆ¤å®š =====
RE_NUMERIC_LIKE = re.compile(r"^[\d\.\,\%\:/xX\+\-\(\)~ï½\smmkKwWhHVVAhLä¸¨Â·â€”â€“]+$")
RE_WAN = re.compile(r"(?P<num>\d+(?:\.\d+)?)\s*ä¸‡")
RE_YUAN = re.compile(r"(?P<num>[\d,]+)\s*å…ƒ")

def is_blank_or_symbol(x) -> bool:
    s = str(x).strip()
    return s in {"", "-", "â€”", "â€”-", "â—", "â—‹"}

def is_numeric_like(x) -> bool:
    return bool(RE_NUMERIC_LIKE.fullmatch(str(x).strip()))

def parse_cny(text: str):
    t = str(text)
    m1 = RE_WAN.search(t)
    if m1:
        return float(m1.group("num")) * 10000.0
    m2 = RE_YUAN.search(t)
    if m2:
        return float(m2.group("num").replace(",", ""))
    return None

def msrp_to_yuan_and_jpy(cell: str, rate: float) -> str:
    t = str(cell).strip()
    if not t or t in {"-", "â€“", "â€”"}:
        return t
    cny = parse_cny(t)
    if cny is None:
        if ("å…ƒ" not in t) and RE_WAN.search(t):
            t = f"{t}å…ƒ"
        return t
    m1 = RE_WAN.search(t)
    yuan_disp = f"{m1.group('num')}ä¸‡å…ƒ" if m1 else (t if "å…ƒ" in t else f"{t}å…ƒ")
    jpy = int(round(cny * rate))
    return f"{yuan_disp}ï¼ˆæ—¥æœ¬å††{jpy:,}å††ï¼‰"

def dealer_to_yuan_only(cell: str) -> str:
    t = str(cell).strip()
    if not t or t in {"-", "â€“", "â€”"}:
        return t
    if ("å…ƒ" not in t) and RE_WAN.search(t):
        t = f"{t}å…ƒ"
    return t

# ===== æ–‡å­—åŒ–ã‘é˜²æ­¢CSVèª­è¾¼ =====
def safe_read_csv(path: Path) -> pd.DataFrame:
    for enc in ("utf-8-sig", "utf-8", "cp932"):
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            continue
    return pd.read_csv(path)

# ===== å°ç‰© =====
def uniq(seq):
    seen, out = set(), []
    for x in seq:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out

def chunked(xs, n):
    for i in range(0, len(xs), n):
        yield xs[i:i+n]

def same_shape_and_headers(df1: pd.DataFrame, df2: pd.DataFrame) -> bool:
    return (df1.shape == df2.shape) and (list(df1.columns) == list(df2.columns))

# ===== ãƒãƒƒãƒç¿»è¨³ï¼ˆæ–°è¦ã«å¿…è¦ãªåˆ†ã ã‘ï¼‰ =====
class Translator:
    def __init__(self, client: OpenAI, model: str, retries=3, sleep_base=1.2, batch_size=60):
        self.client = client
        self.model = model
        self.retries = retries
        self.sleep_base = sleep_base
        self.batch_size = batch_size

    def _translate_chunk(self, terms):
        cleaned = []
        passthrough = {}
        for t in terms:
            if t is None:
                passthrough[""] = ""
                continue
            s = str(t)
            if is_blank_or_symbol(s) or is_numeric_like(s):
                passthrough[s] = s
            else:
                cleaned.append(s)

        out = dict(passthrough)
        if not cleaned:
            return out

        msgs = [
            {"role":"system","content":"ã‚ãªãŸã¯ä¸­å›½èªâ†’æ—¥æœ¬èªã®å°‚é–€ç¿»è¨³è€…ã§ã™ã€‚é…åˆ—ã®å„è¦ç´ ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯ JSON ã®ã¿ã€‚å„è¦ç´ ã¯ {\"cn\": åŸæ–‡, \"ja\": è¨³} ã®é…åˆ—ã§è¿”ã—ã¦ãã ã•ã„ã€‚"},
            {"role":"user","content":json.dumps({"terms":cleaned}, ensure_ascii=False)}
        ]
        resp = self.client.chat.completions.create(
            model=self.model,
            messages=msgs,
            temperature=0,
            response_format={"type":"json_object"},
        )
        content = resp.choices[0].message.content or "{}"
        try:
            data = json.loads(content)
            if isinstance(data, dict) and "translations" in data:
                for item in data["translations"]:
                    cn = str(item.get("cn",""))
                    ja = str(item.get("ja","") or cn)
                    out[cn] = ja
        except Exception:
            for s in cleaned:
                out.setdefault(s, s)
        return out

    def translate_unique(self, unique_terms):
        out = {}
        terms = [str(t) for t in unique_terms if str(t) not in out]
        for chunk in chunked(terms, self.batch_size):
            for attempt in range(1, self.retries+1):
                try:
                    out.update(self._translate_chunk(chunk))
                    break
                except Exception as e:
                    print(f"âš  ãƒãƒƒãƒç¿»è¨³å¤±æ•— ({attempt}/{self.retries}) {e}")
                    if attempt == self.retries:
                        for t in chunk: out.setdefault(str(t), str(t))
                    time.sleep(self.sleep_base * attempt)
        return out

# ===== ãƒ¡ã‚¤ãƒ³ =====
def main():
    series_id = (os.environ.get("SERIES_ID") or "unknown").strip()
    csv_in_env = (os.environ.get("CSV_IN") or "").strip()

    # å…¥åŠ›ï¼šCSV_INå„ªå…ˆã€ç„¡ã‘ã‚Œã° output/autohome/<id>/config_<id>.csv
    if csv_in_env and Path(csv_in_env).exists():
        SRC = Path(csv_in_env)
    else:
        SRC = OUTPUT_DIR / series_id / f"config_{series_id}.csv"

    # å‡ºåŠ›ï¼ˆ448ã¨åŒã˜ï¼‰
    CN_OUT = OUTPUT_DIR / series_id / f"config_{series_id}.csv"           # åŸæ–‡ã‚’ãã®ã¾ã¾
    JA_OUT = OUTPUT_DIR / series_id / f"config_{series_id}.ja.csv"        # è¨³ï¼ˆ_jaåˆ—å«ã‚€ï¼‰
    JA_OUT_COMPAT = OUTPUT_DIR / series_id / f"config_{series_id}_ja.csv" # äº’æ›å

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆã‚·ãƒªãƒ¼ã‚ºæ¯ã«ä¸¸ã”ã¨ä¿å­˜ï¼‰
    CACHE_DIR_SERIES = CACHE_DIR / series_id
    CACHE_DIR_SERIES.mkdir(parents=True, exist_ok=True)  # â† å¿…ãšå…ˆã«ä½œã‚‹
    CACHE_CN = CACHE_DIR_SERIES / "cn.csv"
    CACHE_JA = CACHE_DIR_SERIES / "ja.csv"

    print(f"ğŸ” SRC: {SRC}")
    print(f"ğŸ“ CN:  {CN_OUT}")
    print(f"ğŸ“ JA:  {JA_OUT}")

    if not SRC.exists():
        print(f"âš  å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {SRC}")
        return

    df_cn = safe_read_csv(SRC)
    if df_cn.empty:
        print("âš  å…¥åŠ›CSVãŒç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    # åŸæ–‡ï¼ˆCNï¼‰ã‚’ä¿å­˜ï¼ˆBOMï¼‰
    CN_OUT.parent.mkdir(parents=True, exist_ok=True)
    df_cn.to_csv(CN_OUT, index=False, encoding="utf-8-sig")

    # å‰å›ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆä¸¸ã”ã¨ï¼‰èª­è¾¼
    df_cn_prev = safe_read_csv(CACHE_CN) if CACHE_CN.exists() else None
    df_ja_prev = safe_read_csv(CACHE_JA) if CACHE_JA.exists() else None
    can_reuse = (df_cn_prev is not None) and (df_ja_prev is not None) and same_shape_and_headers(df_cn, df_cn_prev)

    client = OpenAI(api_key=OPENAI_API_KEY)
    tr = Translator(client, OPENAI_MODEL, retries=RETRIES, sleep_base=SLEEP_BASE, batch_size=BATCH_SIZE)

    # å‡ºåŠ›JAãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆCNã‚³ãƒ”ãƒ¼ï¼‹_jaåˆ—è¿½åŠ ï¼‰
    out = df_cn.copy()
    if "ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja" not in out.columns:
        out.insert(1, "ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja", "")
    if "é …ç›®_ja" not in out.columns:
        out.insert(3, "é …ç›®_ja", "")

    # ===== ã‚»ã‚¯ã‚·ãƒ§ãƒ³/é …ç›®ï¼ˆç‰¹åˆ¥æ‰±ã„ã—ãªã„ï¼šã‚»ãƒ«ä½ç½®ã§å·®åˆ†åˆ¤å®šï¼‰ =====
    #   å‰å›CNã¨åŒã˜ä½ç½®ãƒ»å€¤ãªã‚‰å‰å›JAã‚’å†åˆ©ç”¨ã€‚é•ãˆã°ç¿»è¨³ã‚­ãƒ¥ãƒ¼ã¸ã€‚
    sec_terms, itm_terms = [], []
    if can_reuse:
        # å†åˆ©ç”¨
        reuse_mask_sec = df_cn["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"].astype(str).str.strip().values == df_cn_prev["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"].astype(str).str.strip().values
        reuse_mask_itm = df_cn["é …ç›®"].astype(str).str.strip().values == df_cn_prev["é …ç›®"].astype(str).str.strip().values
        out.loc[reuse_mask_sec, "ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"] = df_ja_prev.loc[reuse_mask_sec, "ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"].astype(str).values
        out.loc[reuse_mask_itm, "é …ç›®_ja"]     = df_ja_prev.loc[reuse_mask_itm, "é …ç›®_ja"].astype(str).values
        # å¤‰æ›´ã ã‘ç¿»è¨³å€™è£œã¸
        sec_terms = [str(s).strip() for s, used in zip(out["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"], out["ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"].astype(str).eq("")) if used and str(s).strip()]
        itm_terms = [str(s).strip() for s, used in zip(out["é …ç›®"],     out["é …ç›®_ja"].astype(str).eq("")) if used and str(s).strip()]
    else:
        sec_terms = [str(s).strip() for s in out["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"] if str(s).strip()]
        itm_terms = [str(s).strip() for s in out["é …ç›®"]     if str(s).strip()]

    sec_terms = uniq(sec_terms)
    itm_terms = uniq(itm_terms)
    if sec_terms:
        sec_map = tr.translate_unique(sec_terms)
        out.loc[out["ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"].eq(""), "ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"] = out.loc[out["ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"].eq(""), "ã‚»ã‚¯ã‚·ãƒ§ãƒ³"].map(lambda s: sec_map.get(str(s).strip(), str(s).strip()))
    if itm_terms:
        itm_map = tr.translate_unique(itm_terms)
        out.loc[out["é …ç›®_ja"].eq(""),     "é …ç›®_ja"]     = out.loc[out["é …ç›®_ja"].eq(""),     "é …ç›®"].map(lambda s: itm_map.get(str(s).strip(), str(s).strip()))

    # ===== åˆ—ãƒ˜ãƒƒãƒ€ï¼ˆã‚°ãƒ¬ãƒ¼ãƒ‰åãªã©ï¼‰ =====
    fixed = list(out.columns[:4])
    cur_grades = list(out.columns[4:])
    if can_reuse and list(df_cn_prev.columns) == list(df_cn.columns):
        # åˆ—é…åˆ—ãŒå®Œå…¨ä¸€è‡´ãªã‚‰ã€å‰å›JAã®åˆ—åï¼ˆã‚°ãƒ¬ãƒ¼ãƒ‰åï¼‰ã‚’ãã®ã¾ã¾ä½¿ã†
        out.columns = list(df_ja_prev.columns)
    else:
        grade_terms = uniq([str(c) for c in cur_grades if str(c).strip()])
        if grade_terms:
            grade_map = tr.translate_unique(grade_terms)
            out.columns = fixed + [grade_map.get(str(c), str(c)) for c in cur_grades]
        else:
            out.columns = fixed + cur_grades

    # ===== å€¤ã‚»ãƒ« =====
    MSRP_CN = {"å‚å•†æŒ‡å¯¼ä»·(å…ƒ)", "å‚å•†æŒ‡å¯¼ä»·", "å‚å•†æŒ‡å¯¼ä»·ï¼ˆå…ƒï¼‰"}
    DEALER_CN = {"ç»é”€å•†æŠ¥ä»·", "ç»é”€å•†å‚è€ƒä»·", "ç»é”€å•†"}
    is_msrp   = out["é …ç›®"].isin(MSRP_CN) | out["é …ç›®_ja"].astype(str).str.contains("ãƒ¡ãƒ¼ã‚«ãƒ¼å¸Œæœ›å°å£²ä¾¡æ ¼", na=False)
    is_dealer = out["é …ç›®"].isin(DEALER_CN) | out["é …ç›®_ja"].astype(str).str.contains("ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼è²©å£²ä¾¡æ ¼", na=False)

    # ä¾¡æ ¼æ•´å½¢ï¼ˆã¾ãšä¾¡æ ¼è¡Œã‚’æ•´å½¢ã€éä¾¡æ ¼ã¯å¾Œã§ç¿»è¨³ï¼‰
    for col in out.columns[4:]:
        out.loc[is_msrp, col]   = out.loc[is_msrp, col].map(lambda s: msrp_to_yuan_and_jpy(s, EXRATE))
        out.loc[is_dealer, col] = out.loc[is_dealer, col].map(lambda s: dealer_to_yuan_only(s))

    # éä¾¡æ ¼ã‚»ãƒ«ã®å·®åˆ†ã ã‘ç¿»è¨³å¯¾è±¡ã«ï¼ˆã‚»ãƒ«ä½ç½®ãƒ»å€¤ã§æ¯”è¼ƒï¼‰
    terms_val = []
    if can_reuse and list(df_cn_prev.columns) == list(df_cn.columns):
        for col in out.columns[4:]:
            cn_col = col if can_reuse and (col in df_cn_prev.columns) else None
            if cn_col is None:
                # æ–°ã—ã„åˆ—ã¯å…¨éƒ¨ç¿»è¨³å€™è£œã¸
                vals = out[col].astype(str)
                for v in vals:
                    s = v.strip()
                    if s and (not is_blank_or_symbol(s)) and (not is_numeric_like(s)):
                        terms_val.append(s)
                continue
            cur = df_cn[cn_col].astype(str).str.strip()
            old = df_cn_prev[cn_col].astype(str).str.strip()
            changed = (cur != old)
            for i in out.index:
                if changed.iat[i] and (not is_msrp.iat[i]) and (not is_dealer.iat[i]):
                    v = str(out.iat[i, out.columns.get_loc(col)]).strip()
                    if v and (not is_blank_or_symbol(v)) and (not is_numeric_like(v)):
                        terms_val.append(v)
    else:
        for col in out.columns[4:]:
            for v in out[col].astype(str):
                s = v.strip()
                if s and (not is_blank_or_symbol(s)) and (not is_numeric_like(s)):
                    terms_val.append(s)

    terms_val = uniq(terms_val)
    if terms_val:
        val_map = tr.translate_unique(terms_val)
        for col in out.columns[4:]:
            non_price_mask = ~(is_msrp | is_dealer)
            out.loc[non_price_mask, col] = out.loc[non_price_mask, col].astype(str).map(lambda s: val_map.get(s.strip(), s.strip()))

    # ===== å‡ºåŠ›ï¼ˆBOMï¼‰ =====
    out.to_csv(JA_OUT, index=False, encoding="utf-8-sig")
    out.to_csv(JA_OUT_COMPAT, index=False, encoding="utf-8-sig")

    # ===== ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆã‚·ãƒªãƒ¼ã‚ºä¸¸ã”ã¨ï¼BOMï¼‰ =====
    try:
        df_cn.to_csv(CACHE_CN, index=False, encoding="utf-8-sig")
        out.to_csv(CACHE_JA, index=False, encoding="utf-8-sig")
        print("ğŸ’¾ ãƒªãƒã‚¸ãƒˆãƒªå†…ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°å®Œäº†")
    except Exception as e:
        print(f"âš  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    print("âœ… å®Œäº†ï¼šã‚»ãƒ«å˜ä½ã®å·®åˆ†ã®ã¿ç¿»è¨³ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja/é …ç›®_jaã‚‚ç‰¹åˆ¥æ‰±ã„ãªã—ï¼‰ã€å‰å›ã¨ä¸€è‡´ã‚»ãƒ«ã¯å…¨é¢å†åˆ©ç”¨")

if __name__ == "__main__":
    main()
