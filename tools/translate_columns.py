#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
translate_columns.py

ç›®çš„:
  - Autohome ã®è¨­å®šCSVã‚’æ—¥æœ¬èªåŒ–ã—ã€æœ€çµ‚å‡ºåŠ›(.ja.csv)ã‚’ç”Ÿæˆã™ã‚‹ã€‚
  - å·®åˆ†å†åˆ©ç”¨: ã€Œå‰å›ã®CNã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ(cache/<id>/cn.csv)ã€ã¨ã€Œå‰å›ã®æœ€çµ‚å‡ºåŠ›(.ja.csv)ã€ã‚’çªãåˆã‚ã›ã€
    å¤‰æ›´ã•ã‚Œã¦ã„ãªã„ã‚»ãƒ«ã¯å‰å›ã®JAã‚’ã‚³ãƒ”ãƒ¼ã€å¤‰æ›´ã‚»ãƒ«ã®ã¿ç¿»è¨³ã™ã‚‹ã€‚
  - cache ã¯ CN ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®ã¿ã‚’ä¿å­˜ï¼ˆJAã®åˆ¥æ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ä¿å­˜ã—ãªã„ï¼‰ã€‚
  - å¾Œæ–¹äº’æ›: éå»ã« cache/<id>/ja.csv ãŒã‚ã‚‹å ´åˆã¯å‚ç…§ã¯å¯èƒ½ï¼ˆä¿å­˜ã¯ã—ãªã„ï¼‰ã€‚

å…¥å‡ºåŠ›(ç’°å¢ƒå¤‰æ•°):
  - CSV_IN         : å…¥åŠ›CSV(å¿…é ˆ)
  - CSV_OUT        : äº’æ›ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã€‚ãªã‘ã‚Œã° DST_PRIMARY ã‚’å‚ç…§
  - DST_PRIMARY    : æœ€çµ‚å‡ºåŠ›(æ¨å¥¨)ã€‚CSV_OUTãŒæœªè¨­å®šãªã‚‰ã“ã¡ã‚‰ã‚’å¿…é ˆã¨ã¿ãªã™
  - DST_SECONDARY  : è¿½åŠ å‡ºåŠ›(ä»»æ„)ã€‚æŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°åŒä¸€å†…å®¹ã‚’æ›¸ãå‡ºã™
  - SERIES_ID      : ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å…ˆã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€åã«ä½¿ç”¨ (cache/<SERIES_ID>/)

å‰æã¨ãªã‚‹CSVæ§‹é€ (æœ€ä½é™):
  - åˆ—: ã€Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€ã€Œé …ç›®ã€+ è¤‡æ•°ã®ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ— (CNè¡¨ç¤º)
  - æœ€çµ‚å‡ºåŠ›ã§ã¯: ã€Œã‚»ã‚¯ã‚·ãƒ§ãƒ³_jaã€ã€Œé …ç›®_jaã€ã‚’è¿½åŠ ã—ã€ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã®ä¸­èº«ã¯JAåŒ–ã€åˆ—è¦‹å‡ºã—ã‚‚å¯èƒ½ãªã‚‰JAã¸

ç¿»è¨³ã«ã¤ã„ã¦:
  - æœ¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯å·®åˆ†å†åˆ©ç”¨ã‚’æœ€å„ªå…ˆã€‚ç¿»è¨³å™¨ã¯ã‚ãã¾ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚
  - OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã«é™ã‚Šã€ç°¡æ˜“ã®OpenAI APIå‘¼ã³å‡ºã—ã‚’ã‚µãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã€‚
    æœªè¨­å®š/å¤±æ•—æ™‚ã¯ã€æ’ç­‰(=åŸæ–‡è¿”ã—)ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚
  - å®Ÿé‹ç”¨ã®ç¿»è¨³ã¯æ—¢å­˜ã®ä¸Šæµã‚¹ãƒ†ãƒƒãƒ—/åˆ¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ä»»ã›ã¦OKã€‚ã“ã“ã§ã¯â€œå£Šã•ãªã„ã“ã¨â€ã‚’æœ€å„ªå…ˆã€‚

æ³¨æ„:
  - æ—¢å­˜ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã®äº’æ›ã‚’é‡è¦–ã—ã€åˆ—åãƒ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°(utf-8-sig)ãƒ»ä¾‹å¤–æ™‚æŒ™å‹•ã‚’ä¿å®ˆçš„ã«å®Ÿè£…ã€‚
"""

from __future__ import annotations
import os
import re
import csv
import json
from pathlib import Path
from typing import Optional, Dict, List

import pandas as pd

# ----------------------------
# ç’°å¢ƒå¤‰æ•°ã¨ãƒ‘ã‚¹è§£æ±º
# ----------------------------
SRC = os.environ.get("CSV_IN", "").strip()
DST_PRIMARY = os.environ.get("DST_PRIMARY", "").strip()
CSV_OUT = os.environ.get("CSV_OUT", "").strip()  # äº’æ›
DST_SECONDARY = os.environ.get("DST_SECONDARY", "").strip()
SERIES_ID = os.environ.get("SERIES_ID", "").strip()

if not SRC:
    raise SystemExit("CSV_IN ãŒæœªè¨­å®šã§ã™ã€‚")

if not DST_PRIMARY:
    # äº’æ›: CSV_OUT å„ªå…ˆã€‚ç„¡ã‘ã‚Œã°ã‚¨ãƒ©ãƒ¼
    if CSV_OUT:
        DST_PRIMARY = CSV_OUT
    else:
        raise SystemExit("DST_PRIMARY ã‹ CSV_OUT ã®ã„ãšã‚Œã‹ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

# cache/<id>/cn.csv
def infer_series_id() -> str:
    if SERIES_ID:
        return SERIES_ID
    # å…¥åŠ›CSVãƒ‘ã‚¹ã‹ã‚‰ series_id ã‚’æ¨å®šï¼ˆæ•°å­—é€£ç¶šã‚’å„ªå…ˆï¼‰
    name = Path(SRC).stem
    m = re.search(r"(\d{3,})", name)
    if m:
        return m.group(1)
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåç­‰ã‹ã‚‰ã‚‚è©¦ã™
    m2 = re.search(r"(\d{3,})", str(Path(SRC).parent))
    if m2:
        return m2.group(1)
    return "unknown"

_SERIES = infer_series_id()
CACHE_DIR = Path("cache") / _SERIES
CN_SNAP = CACHE_DIR / "cn.csv"
# å¾Œæ–¹äº’æ›: æ—§æ¥ã®JAã‚­ãƒ£ãƒƒã‚·ãƒ¥(å‚ç…§ã®ã¿) â€»æ–°è¦ä¿å­˜ã¯ã—ãªã„
JA_CACHE_LEGACY = CACHE_DIR / "ja.csv"

# ----------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ----------------------------
def read_csv(path: Path) -> Optional[pd.DataFrame]:
    try:
        return pd.read_csv(path, encoding="utf-8-sig")
    except Exception:
        return None

def write_csv(df: pd.DataFrame, path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(path, index=False, encoding="utf-8-sig")

def norm_cn_cell(x: str) -> str:
    """CNã‚»ãƒ«æ¯”è¼ƒç”¨ã«æ­£è¦åŒ–ï¼ˆç©ºç™½çµ±ä¸€ãƒ»å…¨è§’ç©ºç™½é™¤å»ãƒ»æ”¹è¡Œç­‰ã®ç©ºç™½åŒ–ï¼‰"""
    if x is None:
        return ""
    s = str(x)
    s = s.replace("\u3000", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def same_shape_and_headers(a: pd.DataFrame, b: pd.DataFrame) -> bool:
    if a is None or b is None:
        return False
    if a.shape != b.shape:
        return False
    return list(a.columns) == list(b.columns)

def ensure_required_columns(df: pd.DataFrame) -> None:
    need = ["ã‚»ã‚¯ã‚·ãƒ§ãƒ³", "é …ç›®"]
    for col in need:
        if col not in df.columns:
            raise ValueError(f"å…¥åŠ›CSVã«å¿…é ˆåˆ— {col} ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ã€‚åˆ—å: {list(df.columns)}")

# ----------------------------
# ç¿»è¨³å™¨ï¼ˆæœ€å°é™/ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®‰å…¨ï¼‰
# ----------------------------
_OPENAI_READY = bool(os.environ.get("OPENAI_API_KEY", "").strip())
def translate_text_ja(s: str) -> str:
    """å®‰å…¨ç¬¬ä¸€: æ—¢è¨³å†åˆ©ç”¨ãŒåŠ¹ã‹ãªã‹ã£ãŸæ™‚ã®æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚
       åŸºæœ¬ã¯æ’ç­‰è¿”ã—ï¼ˆå£Šã•ãªã„ï¼‰ã€‚OPENAI_API_KEY ãŒã‚ã‚‹å ´åˆã®ã¿APIè©¦è¡Œã€‚
    """
    s = str(s or "").strip()
    if not s:
        return s
    if not _OPENAI_READY:
        return s  # æ’ç­‰
    try:
        # ã“ã“ã¯â€œä½¿ãˆã‚‹ãªã‚‰ä½¿ã†â€ã«ç•™ã‚ã‚‹ã€‚APIä»•æ§˜ã¯å¤‰ã‚ã‚Šã‚„ã™ã„ã®ã§æœ€å°é™ã€‚
        import requests
        key = os.environ["OPENAI_API_KEY"].strip()
        endpoint = os.environ.get("OPENAI_API_BASE", "https://api.openai.com/v1/chat/completions")
        model = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")
        headers = {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
        prompt = f"æ¬¡ã®ä¸­å›½èªï¼ˆã¾ãŸã¯è‹±èªï¼‰ã‚’æ—¥æœ¬èªã«ç°¡æ½”ã«è¨³ã—ã¦ãã ã•ã„ï¼š\n{s}"
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.2,
        }
        r = requests.post(endpoint, headers=headers, json=payload, timeout=20)
        r.raise_for_status()
        data = r.json()
        cand = data["choices"][0]["message"]["content"].strip()
        return cand or s
    except Exception:
        return s  # å¤±æ•—ã—ã¦ã‚‚å£Šã•ãªã„

# ----------------------------
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ----------------------------
def main():
    # 1) å…¥åŠ›èª­è¾¼
    df = read_csv(Path(SRC))
    if df is None:
        raise SystemExit(f"å…¥åŠ›CSVãŒèª­ã‚ã¾ã›ã‚“: {SRC}")
    ensure_required_columns(df)

    # 2) æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥/å‰å›å‡ºåŠ›ã®å–å¾—
    prev_cn_df = read_csv(CN_SNAP)
    prev_out_df = read_csv(Path(DST_PRIMARY))  # å‰å›ã®æœ€çµ‚å‡ºåŠ›(.ja.csv)
    # å¾Œæ–¹äº’æ›: æ—§æ¥ã® cache/<id>/ja.csv ã‚’å‚ç…§ï¼ˆå­˜åœ¨æ™‚ã®ã¿ï¼‰
    prev_ja_df = read_csv(JA_CACHE_LEGACY)

    # å·®åˆ†å†åˆ©ç”¨ãƒ•ãƒ©ã‚°
    enable_reuse = (prev_cn_df is not None) and same_shape_and_headers(df, prev_cn_df) and (
        (prev_out_df is not None) or (prev_ja_df is not None)
    )

    # 3) å‡ºåŠ›å™¨ã®éª¨æ ¼: out_full ã‚’CNåˆ—ã§åˆæœŸåŒ– â†’ å¾Œã§è¦‹å‡ºã—/ä¸­èº«ã‚’JAã«ç½®æ›
    #    åˆ—æ§‹æˆ: [ã‚»ã‚¯ã‚·ãƒ§ãƒ³, é …ç›®, ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja, é …ç›®_ja] + gradeåˆ—(CNãƒ˜ãƒƒãƒ€ã®ã¾ã¾)
    out_full = pd.DataFrame(index=df.index)
    out_full["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"] = df["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"]
    out_full["é …ç›®"] = df["é …ç›®"]
    out_full["ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"] = ""
    out_full["é …ç›®_ja"] = ""
    grade_cols: List[str] = [c for c in df.columns if c not in ["ã‚»ã‚¯ã‚·ãƒ§ãƒ³", "é …ç›®"]]
    for c in grade_cols:
        out_full[c] = df[c]

    # 4) æ—¢è¨³å†åˆ©ç”¨ãƒãƒƒãƒ—ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³/é …ç›®ï¼‰
    sec_map_old: Dict[str, str] = {}
    item_map_old: Dict[str, str] = {}

    def build_maps_from_prev_out():
        # prev_out_df: [ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja, é …ç›®_ja, grade...], CNåˆ—ã¯ç„¡ã„å‰æ
        # è¡Œå¯¾å¿œã¯ prev_cn_df ã¨ df ãŒå½¢çŠ¶ä¸€è‡´ãªã®ã§ã€åŒã˜ index é †ã§æ¯”è¼ƒOK
        if prev_out_df is None or prev_cn_df is None:
            return
        if ("ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja" not in prev_out_df.columns) or ("é …ç›®_ja" not in prev_out_df.columns):
            return
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        for cur, old_cn, old_ja in zip(df["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"].astype(str),
                                       prev_cn_df["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"].astype(str),
                                       prev_out_df["ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"].astype(str)):
            if norm_cn_cell(cur) == norm_cn_cell(old_cn):
                sec_map_old[str(cur).strip()] = str(old_ja).strip() or str(cur).strip()
        # é …ç›®
        for cur, old_cn, old_ja in zip(df["é …ç›®"].astype(str),
                                       prev_cn_df["é …ç›®"].astype(str),
                                       prev_out_df["é …ç›®_ja"].astype(str)):
            if norm_cn_cell(cur) == norm_cn_cell(old_cn):
                item_map_old[str(cur).strip()] = str(old_ja).strip() or str(cur).strip()

    def build_maps_from_legacy_cache():
        if prev_ja_df is None or prev_cn_df is None:
            return
        if ("ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja" not in prev_ja_df.columns) or ("é …ç›®_ja" not in prev_ja_df.columns):
            return
        for cur, old_cn, old_ja in zip(df["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"].astype(str),
                                       prev_cn_df["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"].astype(str),
                                       prev_ja_df["ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"].astype(str)):
            if norm_cn_cell(cur) == norm_cn_cell(old_cn):
                sec_map_old[str(cur).strip()] = str(old_ja).strip() or str(cur).strip()
        for cur, old_cn, old_ja in zip(df["é …ç›®"].astype(str),
                                       prev_cn_df["é …ç›®"].astype(str),
                                       prev_ja_df["é …ç›®_ja"].astype(str)):
            if norm_cn_cell(cur) == norm_cn_cell(old_cn):
                item_map_old[str(cur).strip()] = str(old_ja).strip() or str(cur).strip()

    if enable_reuse:
        if prev_out_df is not None:
            build_maps_from_prev_out()
        elif prev_ja_df is not None:
            build_maps_from_legacy_cache()

    # 5) ã‚»ã‚¯ã‚·ãƒ§ãƒ³/é …ç›®ã®JAåŸ‹ã‚
    def map_or_translate(d: Dict[str, str], src: str) -> str:
        src = str(src or "").strip()
        if not src:
            return src
        if src in d:
            return d[src]
        # æ—¢è¨³ãŒç„¡ã‘ã‚Œã°æœ€çµ‚æ‰‹æ®µã§ç¿»è¨³å™¨ï¼ˆæ’ç­‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        ja = translate_text_ja(src)
        d[src] = ja
        return ja

    out_full["ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"] = df["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"].map(lambda x: map_or_translate(sec_map_old, x))
    out_full["é …ç›®_ja"]       = df["é …ç›®"].map(lambda x: map_or_translate(item_map_old, x))

    # 6) ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã®ã€Œåˆ—è¦‹å‡ºã—ï¼ˆãƒ˜ãƒƒãƒ€ï¼‰ã€ã®JAå†åˆ©ç”¨
    #    prev_out_df ãŒã‚ã‚Œã°ã€ãã®ãƒ˜ãƒƒãƒ€(ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja/é …ç›®_jaã‚’å«ã‚€)ã‚’è¸è¥²ã™ã‚‹ã®ãŒå®‰å…¨ã€‚
    if enable_reuse and (prev_out_df is not None):
        # prev_out_df: [ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja, é …ç›®_ja, <grade_ja>...]
        # out_full   : [ã‚»ã‚¯ã‚·ãƒ§ãƒ³, é …ç›®, ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja, é …ç›®_ja, <grade_cn>...]
        fixed = list(out_full.columns)[:4]  # ["ã‚»ã‚¯ã‚·ãƒ§ãƒ³", "é …ç›®", "ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja", "é …ç›®_ja"]
        ja_grade_headers = list(prev_out_df.columns)[2:]  # ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja/é …ç›®_ja ã®å¾Œã‚ãŒã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—
        if len(ja_grade_headers) == len(grade_cols):
            out_full.columns = fixed + ja_grade_headers
        # ã‚‚ã—æ•°ãŒåˆã‚ãªã‘ã‚Œã°ãã®ã¾ã¾ï¼ˆå®‰å…¨å„ªå…ˆï¼‰
    else:
        # ãƒ˜ãƒƒãƒ€ç¿»è¨³ã‚’å¼·è¡Œã—ãªã„ï¼ˆå®‰å…¨é‹ç”¨ï¼‰ã€‚å¿…è¦ãªã‚‰ã“ã“ã«ç‹¬è‡ªè¾æ›¸ã‚„æ­£è¦åŒ–ã‚’å·®ã—è¾¼ã‚€ã€‚
        pass

    # 7) ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã®ã€Œã‚»ãƒ«ã®ä¸­èº«ã€å†åˆ©ç”¨/ç¿»è¨³
    #    prev_out_df ãŒã‚ã‚Œã°ã€Œå¤‰æ›´ãªã—ã‚»ãƒ«ã€ã¯ prev_out_df ã‹ã‚‰æµç”¨ã€‚
    if enable_reuse and (prev_cn_df is not None):
        # prev_out_df ãŒå„ªå…ˆ / ãªã‘ã‚Œã° legacy
        ja_source = prev_out_df if prev_out_df is not None else prev_ja_df
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§åŒã˜è¡Œã€åˆ—ä½ç½®ã¯:
        # - prev_out_df ã«ã¯ CNåˆ—ãŒç„¡ã„ã®ã§ã€CNâ†’JAã®åˆ—ã‚·ãƒ•ãƒˆãŒå¿…è¦
        # - prev_ja_df(legacy) ã¯ out_full ã¨åŒã˜åˆ—ã ã£ãŸæƒ³å®šï¼ˆäº’æ›çš„ã«æ‰±ã†ï¼‰
        for i in range(len(df)):
            for j, col in enumerate(grade_cols, start=0):
                cur = norm_cn_cell(df.iat[i, 2 + j])  # df: [ã‚»ã‚¯ã‚·ãƒ§ãƒ³, é …ç›®, grade0, grade1...]
                old = norm_cn_cell(prev_cn_df.iat[i, 2 + j])
                out_col_idx = 4 + j  # out_full: [sec, item, sec_ja, item_ja, grade...]
                if cur == old and ja_source is not None:
                    try:
                        if ja_source is prev_out_df:
                            # prev_out_df ã¯ CNåˆ—ãŒç„¡ã„ã¶ã‚“ã€2åˆ—å·¦ã«è©°ã¾ã£ã¦ã„ã‚‹
                            out_full.iat[i, out_col_idx] = ja_source.iat[i, out_col_idx - 2]
                        else:
                            # legacy: åˆ—æ§‹é€ ãŒ out_full ã¨ä¸€è‡´ã—ã¦ã„ãŸæƒ³å®š
                            out_full.iat[i, out_col_idx] = ja_source.iat[i, out_col_idx]
                        continue
                    except Exception:
                        # ãšã‚ŒãŒã‚ã‚Œã°ç¿»è¨³ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        pass
                # å¤‰æ›´ã‚ã‚Š or å‚ç…§å¤±æ•— â†’ ç¿»è¨³ï¼ˆæ’ç­‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                out_full.iat[i, out_col_idx] = translate_text_ja(df.iat[i, 2 + j])
    else:
        # å†åˆ©ç”¨ä¸å¯ï¼ˆåˆå›/åˆ—å¤‰å‹•ãªã©ï¼‰ â†’ ã™ã¹ã¦ç¿»è¨³ï¼ˆæ’ç­‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        for i in range(len(df)):
            for j, col in enumerate(grade_cols, start=0):
                out_full.iat[i, 4 + j] = translate_text_ja(df.iat[i, 2 + j])

    # 8) æœ€çµ‚å‡ºåŠ›ï¼ˆCNåˆ—ã¯å‡ºåŠ›ã—ãªã„ = è»½é‡ç‰ˆï¼‰
    final_out = out_full[["ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja", "é …ç›®_ja"] + list(out_full.columns[4:])].copy()

    # 9) æ›¸ãå‡ºã—
    write_csv(final_out, Path(DST_PRIMARY))
    if DST_SECONDARY:
        write_csv(final_out, Path(DST_SECONDARY))

    # 10) ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆCNã®ã¿ï¼‰/ JAã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ä¿å­˜ã—ãªã„
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    # å…¥åŠ›ãã®ã‚‚ã®ã‚’ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆCNï¼‰
    write_csv(df, CN_SNAP)

    print(f"âœ… Wrote: {DST_PRIMARY}")
    if DST_SECONDARY:
        print(f"âœ… Wrote: {DST_SECONDARY}")
    print(f"ğŸ“¦ Repo cache CN: {CN_SNAP}")
    # æ—§æ¥: JAã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ä¿å­˜ã—ã¾ã›ã‚“ï¼ˆå‚ç…§ã®ã¿ï¼‰
    # print(f"ğŸ“¦ Repo cache JA: {JA_CACHE_LEGACY} (not saved anymore)")

if __name__ == "__main__":
    main()
