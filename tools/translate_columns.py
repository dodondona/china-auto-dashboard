import os
import re
import time
from pathlib import Path
import pandas as pd
from openai import OpenAI
import json

# ====== å®šæ•°ãƒ»è¨­å®š ======
CACHE_DIR = Path("cache_repo/series")   # ãƒªãƒã‚¸ãƒˆãƒªå†…ã‚­ãƒ£ãƒƒã‚·ãƒ¥
OUTPUT_DIR = Path("output/autohome")    # å‡ºåŠ›å…ˆ
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")

RETRIES = 3
SLEEP_BASE = 1.2
BATCH_SIZE = 60
EXRATE = float(os.environ.get("EXRATE_CNY_TO_JPY", "21.0"))

# è¨˜å·ãƒ»æ•°å€¤åˆ¤å®š
RE_NUMERIC_LIKE = re.compile(r"^[\d\.\,\%\:/xX\+\-\(\)~ï½\smmkKwWhHVVAhLä¸¨Â·â€”â€“]+$")
def is_blank_or_symbol(x) -> bool:
    s = str(x).strip()
    return s in {"", "-", "â€”", "â€”-", "â—", "â—‹"}
def is_numeric_like(x) -> bool:
    return bool(RE_NUMERIC_LIKE.fullmatch(str(x).strip()))

# ä¾¡æ ¼ç³»
RE_WAN = re.compile(r"(?P<num>\d+(?:\.\d+)?)\s*ä¸‡")
RE_YUAN = re.compile(r"(?P<num>[\d,]+)\s*å…ƒ")
def parse_cny(text: str):
    t = str(text)
    m1 = RE_WAN.search(t)
    if m1:
        return float(m1.group("num")) * 10000.0
    m2 = RE_YUAN.search(t)
    if m2:
        return float(m2.group("num").replace(",", ""))
    return None
def msrp_to_yuan_and_jpy(cell: str, rate: float) -> str:
    t = str(cell).strip()
    if not t or t in {"-", "â€“", "â€”"}:
        return t
    cny = parse_cny(t)
    if cny is None:
        if ("å…ƒ" not in t) and RE_WAN.search(t):
            t = f"{t}å…ƒ"
        return t
    m1 = RE_WAN.search(t)
    yuan_disp = f"{m1.group('num')}ä¸‡å…ƒ" if m1 else (t if "å…ƒ" in t else f"{t}å…ƒ")
    jpy = int(round(cny * rate))
    return f"{yuan_disp}ï¼ˆæ—¥æœ¬å††{jpy:,}å††ï¼‰"
def dealer_to_yuan_only(cell: str) -> str:
    t = str(cell).strip()
    if not t or t in {"-", "â€“", "â€”"}:
        return t
    if ("å…ƒ" not in t) and RE_WAN.search(t):
        t = f"{t}å…ƒ"
    return t

# CSV èª­è¾¼ï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰
def safe_read_csv(path: Path) -> pd.DataFrame:
    for enc in ("utf-8-sig", "utf-8", "cp932"):
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            continue
    return pd.read_csv(path)

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
def uniq(seq):
    seen, out = set(), []
    for x in seq:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out

def chunked(xs, n):
    for i in range(0, len(xs), n):
        yield xs[i:i+n]

def same_shape_and_headers(df1: pd.DataFrame, df2: pd.DataFrame) -> bool:
    return (df1.shape == df2.shape) and (list(df1.columns) == list(df2.columns))

# ====== ãƒãƒƒãƒç¿»è¨³ï¼ˆæ–°è¦ã«å¿…è¦ãªåˆ†ã ã‘ï¼‰ ======
class Translator:
    def __init__(self, client: OpenAI, model: str, retries=3, sleep_base=1.2, batch_size=60):
        self.client = client
        self.model = model
        self.retries = retries
        self.sleep_base = sleep_base
        self.batch_size = batch_size

    def _translate_chunk(self, terms):
        # å‹å®‰å…¨åŒ–ï¼†ã‚¹ã‚­ãƒƒãƒ—ï¼ˆè¨˜å·ãƒ»æ•°å€¤ï¼‰ã¯ã“ã“ã§ã‚‚ä¸‡å…¨ã«
        cleaned = []
        passthrough = {}
        for t in terms:
            if t is None:
                passthrough[""] = ""
                continue
            s = str(t)
            if is_blank_or_symbol(s) or is_numeric_like(s):
                passthrough[s] = s
            else:
                cleaned.append(s)

        out = dict(passthrough)
        if not cleaned:
            return out

        msgs = [
            {"role":"system","content":"ã‚ãªãŸã¯ä¸­å›½èªâ†’æ—¥æœ¬èªã®å°‚é–€ç¿»è¨³è€…ã§ã™ã€‚é…åˆ—ã®å„è¦ç´ ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯ JSON ã®ã¿"},
            {"role":"user","content":json.dumps({"terms":cleaned}, ensure_ascii=False)}
        ]
        resp = self.client.chat.completions.create(
            model=self.model,
            messages=msgs,
            temperature=0,
            response_format={"type":"json_object"},
        )
        content = resp.choices[0].message.content or "{}"
        try:
            data = json.loads(content)
            if isinstance(data, dict) and "translations" in data:
                for item in data["translations"]:
                    cn = str(item.get("cn",""))
                    ja = str(item.get("ja","") or cn)
                    out[cn] = ja
        except Exception:
            for s in cleaned:
                out.setdefault(s, s)
        return out

    def translate_unique(self, unique_terms):
        out = {}
        terms = [str(t) for t in unique_terms if str(t) not in out]
        for chunk in chunked(terms, self.batch_size):
            for attempt in range(1, self.retries+1):
                try:
                    out.update(self._translate_chunk(chunk))
                    break
                except Exception as e:
                    print(f"âš  ãƒãƒƒãƒç¿»è¨³å¤±æ•— ({attempt}/{self.retries}) {e}")
                    if attempt == self.retries:
                        for t in chunk: out.setdefault(str(t), str(t))
                    time.sleep(self.sleep_base * attempt)
        return out

# ====== ãƒ¡ã‚¤ãƒ³ ======
def main():
    series_id = (os.environ.get("SERIES_ID") or "unknown").strip()
    csv_in_env = (os.environ.get("CSV_IN") or "").strip()

    # å…¥åŠ›ï¼šCSV_INå„ªå…ˆã€ãªã‘ã‚Œã° output/autohome/<id>/config_<id>.csv
    if csv_in_env and Path(csv_in_env).exists():
        SRC = Path(csv_in_env)
    else:
        SRC = OUTPUT_DIR / series_id / f"config_{series_id}.csv"

    CN_OUT = OUTPUT_DIR / series_id / f"config_{series_id}.csv"           # åŸæ–‡
    JA_OUT = OUTPUT_DIR / series_id / f"config_{series_id}.ja.csv"        # ç¿»è¨³
    JA_OUT_COMPAT = OUTPUT_DIR / series_id / f"config_{series_id}_ja.csv" # äº’æ›

    CACHE_CN = CACHE_DIR / series_id / "cn.csv"
    CACHE_JA = CACHE_DIR / series_id / "ja.csv"

    print(f"ğŸ” SRC: {SRC}")
    print(f"ğŸ“ CN:  {CN_OUT}")
    print(f"ğŸ“ JA:  {JA_OUT}")

    if not SRC.exists():
        print(f"âš  å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {SRC}")
        return

    df_cn = safe_read_csv(SRC)
    if df_cn.empty:
        print("âš  å…¥åŠ›CSVãŒç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    # åŸæ–‡ã‚’ãã®ã¾ã¾ä¿å­˜ï¼ˆBOMï¼‰
    CN_OUT.parent.mkdir(parents=True, exist_ok=True)
    df_cn.to_csv(CN_OUT, index=False, encoding="utf-8-sig")

    # å‰å›ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­è¾¼
    df_cn_prev = safe_read_csv(CACHE_CN) if CACHE_CN.exists() else None
    df_ja_prev = safe_read_csv(CACHE_JA) if CACHE_JA.exists() else None
    can_reuse = (df_cn_prev is not None) and (df_ja_prev is not None) and same_shape_and_headers(df_cn, df_cn_prev)

    client = OpenAI(api_key=OPENAI_API_KEY)
    tr = Translator(client, OPENAI_MODEL, retries=RETRIES, sleep_base=SLEEP_BASE, batch_size=BATCH_SIZE)

    # å‡ºåŠ›ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆ448ã¨åŒæ§‹æˆï¼‰
    out = df_cn.copy()
    if "ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja" not in out.columns:
        out.insert(1, "ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja", "")
    if "é …ç›®_ja" not in out.columns:
        out.insert(3, "é …ç›®_ja", "")

    # ---- ã‚»ã‚¯ã‚·ãƒ§ãƒ³/é …ç›®ï¼šå‰å›CNã¨ä¸€è‡´ãªã‚‰JAã‚’å†åˆ©ç”¨ã€‚æ–°è¦ãƒ»å¤‰æ›´ã®ã¿ãƒãƒƒãƒç¿»è¨³ ----
    if can_reuse:
        # æœªå¤‰æ›´ã‚»ãƒ«ã¯å‰å›JAã‚’ã‚³ãƒ”ãƒ¼
        mask_sec = (df_cn["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"].astype(str).str.strip().values == df_cn_prev["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"].astype(str).str.strip().values) & df_ja_prev["ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"].astype(str).ne("").values
        mask_itm = (df_cn["é …ç›®"].astype(str).str.strip().values == df_cn_prev["é …ç›®"].astype(str).str.strip().values) & df_ja_prev["é …ç›®_ja"].astype(str).ne("").values
        out.loc[mask_sec, "ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"] = df_ja_prev.loc[mask_sec, "ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"].values
        out.loc[mask_itm, "é …ç›®_ja"]     = df_ja_prev.loc[mask_itm, "é …ç›®_ja"].values

        # å¤‰æ›´åˆ†ã ã‘ã‚’ãƒ¦ãƒ‹ãƒ¼ã‚¯æŠ½å‡º
        sec_terms = uniq([str(s).strip() for s, used in zip(out["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"], out["ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"].astype(str).eq("")) if used and str(s).strip()])
        itm_terms = uniq([str(s).strip() for s, used in zip(out["é …ç›®"],     out["é …ç›®_ja"].astype(str).eq("")) if used and str(s).strip()])
    else:
        sec_terms = uniq([str(s).strip() for s in out["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"] if str(s).strip()])
        itm_terms = uniq([str(s).strip() for s in out["é …ç›®"]     if str(s).strip()])

    sec_map = tr.translate_unique(sec_terms) if sec_terms else {}
    itm_map = tr.translate_unique(itm_terms) if itm_terms else {}

    # åŸ‹ã‚è¾¼ã¿
    out.loc[out["ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"].eq(""), "ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"] = out.loc[out["ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja"].eq(""), "ã‚»ã‚¯ã‚·ãƒ§ãƒ³"].map(lambda s: sec_map.get(str(s).strip(), str(s).strip()))
    out.loc[out["é …ç›®_ja"].eq(""),     "é …ç›®_ja"]     = out.loc[out["é …ç›®_ja"].eq(""),     "é …ç›®"].map(lambda s: itm_map.get(str(s).strip(), str(s).strip()))

    # ---- åˆ—ãƒ˜ãƒƒãƒ€ï¼ˆã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰ï¼šCNé…åˆ—ãŒå‰å›ã¨å®Œå…¨ä¸€è‡´ãªã‚‰å‰å›JAã‚’æµç”¨ã€é•ãˆã°æ–°è¦ãƒãƒƒãƒç¿»è¨³ ----
    fixed = list(out.columns[:4])
    cur_grades = list(out.columns[4:])
    if can_reuse and list(df_cn_prev.columns[4:]) == list(df_cn.columns[4:]) and list(df_ja_prev.columns[:4]) == fixed:
        out.columns = list(df_ja_prev.columns)  # å‰å›JAã‚’ä¸¸ã”ã¨æµç”¨
    else:
        grade_terms = uniq([str(c) for c in cur_grades])
        grade_map = tr.translate_unique(grade_terms)
        out.columns = fixed + [grade_map.get(str(c), str(c)) for c in cur_grades]

    # ---- å€¤ã‚»ãƒ«ï¼šä¾¡æ ¼æ•´å½¢ â†’ éä¾¡æ ¼ã‚»ãƒ«ã®ã†ã¡ â€œå¤‰æ›´ã‚»ãƒ«ã®ã¿â€ ã‚’ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ã—ã¦ãƒãƒƒãƒç¿»è¨³ ----
    MSRP_CN = {"å‚å•†æŒ‡å¯¼ä»·(å…ƒ)", "å‚å•†æŒ‡å¯¼ä»·", "å‚å•†æŒ‡å¯¼ä»·ï¼ˆå…ƒï¼‰"}
    DEALER_CN = {"ç»é”€å•†æŠ¥ä»·", "ç»é”€å•†å‚è€ƒä»·", "ç»é”€å•†"}
    is_msrp   = out["é …ç›®"].isin(MSRP_CN) | out["é …ç›®_ja"].astype(str).str.contains("ãƒ¡ãƒ¼ã‚«ãƒ¼å¸Œæœ›å°å£²ä¾¡æ ¼", na=False)
    is_dealer = out["é …ç›®"].isin(DEALER_CN) | out["é …ç›®_ja"].astype(str).str.contains("ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼è²©å£²ä¾¡æ ¼", na=False)

    # ä¾¡æ ¼æ•´å½¢
    for col in out.columns[4:]:
        out.loc[is_msrp, col]   = out.loc[is_msrp, col].map(lambda s: msrp_to_yuan_and_jpy(s, EXRATE))
        out.loc[is_dealer, col] = out.loc[is_dealer, col].map(lambda s: dealer_to_yuan_only(s))

    # éä¾¡æ ¼ãƒ»å¤‰æ›´ã‚»ãƒ«ã ã‘é›†ã‚ã‚‹
    terms_val = []
    if can_reuse:
        for col in out.columns[4:]:
            cur = df_cn[col].astype(str).str.strip()
            old = df_cn_prev[col].astype(str).str.strip()
            changed = (cur != old)
            for i in out.index:
                if changed.iat[i] and (not is_msrp.iat[i]) and (not is_dealer.iat[i]):
                    v = str(out.iat[i, out.columns.get_loc(col)]).strip()
                    if v and (not is_blank_or_symbol(v)) and (not is_numeric_like(v)):
                        terms_val.append(v)
    else:
        for col in out.columns[4:]:
            for v in out[col].astype(str):
                s = v.strip()
                if s and (not is_blank_or_symbol(s)) and (not is_numeric_like(s)):
                    terms_val.append(s)

    terms_val = uniq(terms_val)
    val_map = tr.translate_unique(terms_val) if terms_val else {}

    for col in out.columns[4:]:
        non_price_mask = ~(is_msrp | is_dealer)
        out.loc[non_price_mask, col] = out.loc[non_price_mask, col].astype(str).map(lambda s: val_map.get(s.strip(), s.strip()))

    # ---- å‡ºåŠ›ï¼ˆBOMï¼‰ ----
    out.to_csv(JA_OUT, index=False, encoding="utf-8-sig")
    out.to_csv(JA_OUT_COMPAT, index=False, encoding="utf-8-sig")

    # ---- ãƒªãƒã‚¸ãƒˆãƒªå†…ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆBOMï¼‰ ----
    try:
        cols_cn = [c for c in ["ã‚»ã‚¯ã‚·ãƒ§ãƒ³", "é …ç›®"] if c in df_cn.columns]
        if cols_cn:
            df_cn[cols_cn].to_csv(CACHE_CN, index=False, encoding="utf-8-sig")
        cols_ja = [c for c in ["ã‚»ã‚¯ã‚·ãƒ§ãƒ³", "é …ç›®", "ã‚»ã‚¯ã‚·ãƒ§ãƒ³_ja", "é …ç›®_ja"] if c in out.columns]
        if cols_ja:
            out[cols_ja].to_csv(CACHE_JA, index=False, encoding="utf-8-sig")
        print("ğŸ’¾ ãƒªãƒã‚¸ãƒˆãƒªå†…ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°å®Œäº†")
    except Exception as e:
        print(f"âš  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    print("âœ… å®Œäº†ï¼ˆã‚»ãƒ«å·®åˆ†ï¼‹å‰å›JAå†åˆ©ç”¨ã€é‡è¤‡ã¯æ–°è¦åˆ†ã®ã¿ãƒãƒƒãƒç¿»è¨³ï¼‰")

if __name__ == "__main__":
    main()
