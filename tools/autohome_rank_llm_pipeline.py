#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
autohome_rank_llm_pipeline.py
- /rank/1 ã®HTMLã‚’HTTPã§å–å¾—ï¼ˆJSä¸è¦ï¼‰
- å‡ºç¾é †ã§ series_id ã‚’æŠ½å‡ºï¼ˆ=é †ä½ï¼‰
- å„ series_url ã® <title> ã‚’HTTPã§å–å¾—
- title ã‚’ LLM (gpt-4o-mini) ã§è§£æã—ã¦ brand/model ã‚’æŠ½å‡º
- CSV: data/autohome_raw_YYYY-MM_with_brand.csv ã‚’ä¿å­˜

ä¾å­˜: requests, pandas, beautifulsoup4, openai
"""

import os
import re
import time
import json
import argparse
from datetime import datetime
from pathlib import Path

import requests
import pandas as pd
from bs4 import BeautifulSoup
from openai import OpenAI

RANK_URL_DEFAULT = "https://www.autohome.com.cn/rank/1"
UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/124.0.0.0 Safari/537.36"
)
HDRS = {
    "User-Agent": UA,
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,ja;q=0.7",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
}

PROMPT_JSON = (
    "ä½ å°†çœ‹åˆ°ä¸€ä¸ªä¸­å›½æ±½è½¦ä¹‹å®¶è½¦ç³»é¡µé¢çš„æ ‡é¢˜ï¼Œè¯·è§£æå¹¶è¾“å‡º JSONï¼š"
    "{\"brand\":\"å“ç‰Œå\",\"model\":\"è½¦ç³»å\"}ã€‚"
    "å¦‚æœæ— æ³•åˆ¤æ–­åˆ™ä½¿ç”¨ç©ºå­—ç¬¦ä¸²ã€‚åªè¾“å‡º JSONï¼Œä¸è¦å¤šä½™æ–‡å­—ã€‚"
)

def fetch_html(url: str, retries: int = 4, timeout: int = 20) -> str:
    last_exc = None
    for i in range(retries):
        try:
            resp = requests.get(url, headers=HDRS, timeout=timeout)
            # ä¸€éƒ¨ãƒšãƒ¼ã‚¸ã¯ GBK/GB2312ã€‚requestsã®è‡ªå‹•åˆ¤å®šã ã¨å´©ã‚Œã‚‹ã®ã§æ‰‹å½“ã¦ã€‚
            enc = resp.apparent_encoding or "utf-8"
            resp.encoding = enc
            if resp.status_code == 200 and resp.text:
                return resp.text
        except Exception as e:
            last_exc = e
        time.sleep(1.2 + i * 0.8)
    if last_exc:
        raise last_exc
    raise RuntimeError(f"Failed to fetch: {url}")

def extract_series_ids(html: str, max_items: int = 60) -> list[str]:
    """data-series-id å„ªå…ˆ â†’ hrefãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚å‡ºç¾é †ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã€‚"""
    ids = re.findall(r'data-series-id\s*=\s*"(\d+)"', html)
    if not ids:
        # //www.autohome.com.cn/1234/ ã¾ãŸã¯ https://... ã®ä¸¡æ–¹ã«å¯¾å¿œ
        ids = re.findall(r'href="(?:https:)?//www\.autohome\.com\.cn/(\d{3,7})/?[^"]*"', html, flags=re.I)
    uniq = []
    seen = set()
    for sid in ids:
        if sid not in seen:
            seen.add(sid)
            uniq.append(sid)
    return uniq[:max_items]

def extract_counts_heuristic(html: str, series_ids: list[str]) -> dict[str, int|None]:
    """è¡Œã®è¿‘å‚ã‹ã‚‰ 'è½¦ç³»é”€é‡' ã®æ•°å­—ã‚’æ‹¾ã†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã€‚ãªã‘ã‚Œã° Noneã€‚"""
    res = {}
    for sid in series_ids:
        idx = html.find(sid)
        val = None
        if idx != -1:
            chunk = html[max(0, idx - 800): idx + 800]
            m = re.search(r'(\d{4,6})\s*è½¦ç³»é”€é‡', chunk)
            if m:
                try:
                    val = int(m.group(1))
                except Exception:
                    val = None
        res[sid] = val
    return res

def fetch_title(url: str) -> str:
    try:
        html = fetch_html(url, retries=3, timeout=15)
        soup = BeautifulSoup(html, "html.parser")
        # é€šå¸¸ <title>â€¦</title>
        t = soup.title.string.strip() if soup.title and soup.title.string else ""
        if not t:
            # äºˆå‚™ï¼š<meta property="og:title"> ç­‰
            m = soup.find("meta", attrs={"property": "og:title"}) or soup.find("meta", attrs={"name": "title"})
            if m and m.get("content"):
                t = m["content"].strip()
        return t
    except Exception:
        return ""

def llm_brand_model(client: OpenAI, title: str, model_name: str = "gpt-4o-mini") -> tuple[str, str]:
    if not title:
        return ("", "")
    try:
        resp = client.chat.completions.create(
            model=model_name,
            temperature=0,
            max_tokens=120,
            messages=[
                {"role": "system", "content": PROMPT_JSON},
                {"role": "user", "content": title},
            ],
        )
        text = (resp.choices[0].message.content or "").strip()
        # JSONã ã‘æŠ½å‡ºï¼ˆä¿é™ºï¼‰
        m = re.search(r"\{.*\}", text, re.S)
        data = json.loads(m.group(0)) if m else {}
        brand = (data.get("brand") or "").strip()
        model = (data.get("model") or "").strip()
        return (brand, model)
    except Exception:
        return ("", "")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--rank-url", default=RANK_URL_DEFAULT)
    ap.add_argument("--output", default=f"data/autohome_raw_{datetime.now():%Y-%m}_with_brand.csv")
    ap.add_argument("--max-items", type=int, default=60)
    ap.add_argument("--model", default="gpt-4o-mini")
    args = ap.parse_args()

    Path("data").mkdir(parents=True, exist_ok=True)
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    # 1) rankãƒšãƒ¼ã‚¸HTML
    print(f"ğŸŒ GET {args.rank_url}")
    rank_html = fetch_html(args.rank_url)
    # ãƒ‡ãƒãƒƒã‚°ä¿å­˜ï¼ˆä½•ã‹ã‚ã£ãŸæ™‚ã«è¦‹è¿”ã›ã‚‹ï¼‰
    Path("data/_rankpage_debug.html").write_text(rank_html, encoding="utf-8", errors="ignore")

    # 2) å‡ºç¾é †ã§ series_id ã‚’æŠ½å‡ºï¼ˆ= é †ä½ï¼‰
    sids = extract_series_ids(rank_html, max_items=args.max_items)
    if not sids:
        raise SystemExit("âŒ series_id ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆWAF/æ§‹é€ å¤‰æ›´ã®å¯èƒ½æ€§ï¼‰")

    counts = extract_counts_heuristic(rank_html, sids)

    # 3) å„ series_url ã® <title> ã‚’å–å¾—
    rows = []
    for i, sid in enumerate(sids, start=1):
        url = f"https://www.autohome.com.cn/{sid}/"
        title = fetch_title(url)
        # 4) LLMã§ brand / model ã‚’è§£æ
        brand, model = llm_brand_model(client, title, model_name=args.model)
        rows.append({
            "rank": i,
            "series_url": url,
            "title": title,
            "brand": brand,
            "model": model,
            "count": counts.get(sid)
        })
        # ã‚µã‚¤ãƒˆè² è·è»½æ¸›
        time.sleep(0.25)

    df = pd.DataFrame(rows).sort_values("rank").reset_index(drop=True)
    out = Path(args.output)
    df.to_csv(out, index=False, encoding="utf-8-sig")
    print(f"âœ… Saved: {out} ({len(df)} rows)")

if __name__ == "__main__":
    main()
