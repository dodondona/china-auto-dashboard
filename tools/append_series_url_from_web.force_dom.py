#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Append series_url (and optionally count) to CSV by scraping /rank/1.

æœ€å°å¤‰æ›´ã§æ—¢å­˜ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ã¯ã‚è¾¼ã‚ã‚‹ãƒ‰ãƒ­ãƒƒãƒ—ã‚¤ãƒ³ç‰ˆ:
- å¼•æ•°ã¯å¾“æ¥ã©ãŠã‚Š(--rank-url --input --output --name-col --idle-ms --max-rounds)
- rank ã¯ DOM å‡ºç¾é †ï¼ˆ=ç”»é¢ã®ä¸¦ã³ï¼‰
- ã¾ãš button[data-series-id] ã‚’ä½¿ã†ã€‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ <a href="//www.autohome.com.cn/<digits>/"> ã‚’æ­£è¦è¡¨ç¾ã§å›å
- wait_for_selector ã¯ä½¿ã‚ãšã€querySelectorAll ã® length ã‚’ãƒãƒ¼ãƒªãƒ³ã‚°ï¼ˆå¯è¦–åŒ–å¾…ã¡ã‚’å›é¿ï¼‰
"""

import os
import re
import argparse
from pathlib import Path
import pandas as pd
from playwright.sync_api import sync_playwright

def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--rank-url", default="https://www.autohome.com.cn/rank/1")
    ap.add_argument("--input", required=True)
    ap.add_argument("--output", required=True)
    ap.add_argument("--name-col", default="model")  # äº’æ›ç›®çš„ï¼ˆæœªä½¿ç”¨ï¼‰
    ap.add_argument("--idle-ms", type=int, default=650)
    ap.add_argument("--max-rounds", type=int, default=40)
    args, _ = ap.parse_known_args()
    return args

UA_MOBILE = (
    "Mozilla/5.0 (Linux; Android 11; Pixel 5) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/122.0.0.0 Mobile Safari/537.36"
)

def safe_int(x):
    try:
        return int(str(x).strip())
    except Exception:
        return None

def poll_series_button_count(page, total_ms=180000, step_ms=800):
    """
    å¯è¦–çŠ¶æ…‹ã‚’å¾…ãŸãšã€DOMã«ç¾ã‚ŒãŸå€‹æ•°ã‚’ãƒãƒ¼ãƒªãƒ³ã‚°ã€‚
    ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚‚ã—ãªãŒã‚‰å®‰å®šã™ã‚‹ã¾ã§å¾…ã¤ã€‚
    """
    waited = 0
    stable = 0
    last = -1
    while waited < total_ms:
        n = page.evaluate("() => document.querySelectorAll('button[data-series-id]').length")
        if n > 0 and n == last:
            stable += 1
        else:
            stable = 0
        last = n
        if n > 0 and stable >= 2:   # 2å›é€£ç¶šã§å¤‰åŒ–ãªã—ï¼å®‰å®š
            return n
        page.mouse.wheel(0, 22000)
        page.wait_for_timeout(step_ms)
        waited += step_ms
    return 0

def scroll_to_bottom(page, idle_ms=650, max_rounds=40):
    prev = -1
    stable = 0
    for _ in range(max_rounds):
        page.mouse.wheel(0, 24000)
        page.wait_for_timeout(idle_ms)
        n = page.evaluate("() => document.querySelectorAll('button[data-series-id]').length")
        if n == prev:
            stable += 1
        else:
            stable = 0
        prev = n
        if stable >= 3:
            break
    return prev

def scrape_by_buttons(page):
    """button[data-series-id] ãŒä½¿ãˆã‚‹ã¨ãã®çµŒè·¯ï¼ˆä»¥å‰ã¨åŒã˜æŒ™å‹•ï¼‰ã€‚"""
    buttons = page.query_selector_all("button[data-series-id]") or []
    rows = []
    for idx, btn in enumerate(buttons, start=1):
        sid = btn.get_attribute("data-series-id")
        url = f"https://www.autohome.com.cn/{sid}/" if sid else None
        # count ã¯å¿…è¦ãªã‚‰ã“ã“ã§è¡Œãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ‹¾ã†ï¼ˆäº’æ›ã®ãŸã‚ None ã§ã‚‚OKï¼‰
        rows.append({"rank": idx, "series_url": url})
    return rows

def scrape_by_anchor_regex(page):
    """
    ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çµŒè·¯:
    ãƒšãƒ¼ã‚¸HTMLã‹ã‚‰ <a href="//www.autohome.com.cn/<digits>[/#?]..."> ã‚’
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé †ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯æŠ½å‡ºï¼ˆæœ€åˆã®50ï½60ä»¶ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®æœ¬ä½“ï¼‰ã€‚
    """
    html = page.content()
    # href ã¯ // ã‹ã‚‰å§‹ã¾ã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ https: ã‚’è£œã†
    pattern = re.compile(r'href="(?:https:)?//www\.autohome\.com\.cn/(\d{3,7})/?(?:[?#"][^"]*)?"')
    seen, ids = set(), []
    for m in pattern.finditer(html):
        sid = m.group(1)
        if sid not in seen:
            seen.add(sid)
            ids.append(sid)
    rows = [{"rank": i, "series_url": f"https://www.autohome.com.cn/{sid}/"} for i, sid in enumerate(ids, start=1)]
    # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ€ãƒ³ãƒ—ï¼ˆå¿µã®ãŸã‚ï¼‰
    Path("data").mkdir(parents=True, exist_ok=True)
    with open("data/debug_rankpage_fallback.html", "w", encoding="utf-8") as f:
        f.write(html)
    return rows

def scrape_rank_list(url, idle_ms, max_rounds):
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=["--disable-blink-features=AutomationControlled"]
        )
        ctx = browser.new_context(
            user_agent=UA_MOBILE,
            viewport={"width": 480, "height": 960},
            locale="zh-CN",
            timezone_id="Asia/Shanghai",
            extra_http_headers={
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Cache-Control": "no-cache",
            },
        )
        page = ctx.new_page()
        page.goto(url, wait_until="domcontentloaded", timeout=180000)

        # å¯è¦–å¾…ã¡ã‚’ã‚„ã‚ã¦ãƒãƒ¼ãƒªãƒ³ã‚°
        _ = poll_series_button_count(page, total_ms=180000, step_ms=800)
        _ = scroll_to_bottom(page, idle_ms=idle_ms, max_rounds=max_rounds)

        # ã¾ãšã¯ã€Œå‰ã¨åŒã˜ã€ãƒœã‚¿ãƒ³çµŒè·¯
        rows = scrape_by_buttons(page)

        # 0ä»¶ã ã£ãŸã‚‰ã‚¢ãƒ³ã‚«ãƒ¼æ­£è¦è¡¨ç¾ã§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æŠ½å‡º
        if not rows:
            rows = scrape_by_anchor_regex(page)

        browser.close()
    return rows

def main():
    args = parse_args()
    inp = Path(args.input)
    out = Path(args.output)
    out.parent.mkdir(parents=True, exist_ok=True)

    df = pd.read_csv(inp, encoding="utf-8-sig")
    if "rank" not in df.columns:
        # raw ãŒ rank ç„¡ã—ã§ã‚‚å®‰å…¨ã«å‹•ãã‚ˆã†ã«
        df.insert(0, "rank", range(1, len(df) + 1))

    print(f"ğŸ“¥ input: {inp} ({len(df)} rows)")
    print(f"ğŸŒ scraping: {args.rank_url}")

    web_rows = scrape_rank_list(args.rank_url, args.idle_ms, args.max_rounds)
    web = pd.DataFrame(web_rows)

    # rank ã§ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆã«çµåˆï¼ˆåå‰ã¯ä½¿ã‚ãªã„ï¼å–ã‚Šé•ã„ã‚’é¿ã‘ã‚‹ï¼‰
    merged = df.merge(web, on="rank", how="left")

    # series_url åˆ—ã®æ­£è¦åŒ–
    if "series_url_y" in merged.columns and "series_url_x" in merged.columns:
        merged["series_url"] = merged["series_url_x"].fillna(merged["series_url_y"])
        merged = merged.drop(columns=["series_url_x", "series_url_y"])
    elif "series_url" not in merged.columns and "series_url_y" in merged.columns:
        merged = merged.rename(columns={"series_url_y": "series_url"})

    merged = merged.sort_values("rank").reset_index(drop=True)
    merged.to_csv(out, index=False, encoding="utf-8-sig")
    print(f"âœ… output: {out} ({len(merged)} rows)")

if __name__ == "__main__":
    main()
