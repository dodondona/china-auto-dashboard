#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Autohome ã® /rank/ ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚·ãƒªãƒ¼ã‚ºè©³ç´° URL ã‚’å–å¾—ã—ã€å…¥åŠ›CSVã«
series_url ã‚’ä»˜ä¸ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã€‚

ãƒ»Playwright (Chromium) ã§ã¾ãšå–å¾—
ãƒ»å¤±æ•—/ç©ºã®ã¨ãã¯ requests + HTML ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
ãƒ»ãƒšãƒ¼ã‚¸ä¸­ã® https://www.autohome.com.cn/<digits>/ ã‚’å…¨åˆ—æŒ™ã—ã€å‡ºç¾é †ã‚’ rank=1..N ã«æ¡ç•ª
ãƒ»å…¥åŠ›å´ã¯ rank / rank_seq ã®ã©ã¡ã‚‰ã§ã‚‚è‡ªå‹•å¯¾å¿œ
ãƒ»ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœãŒç©ºã§ã‚‚è½ã¨ã•ãšç´ é€šã—ï¼ˆseries_url ã‚’æ¬ æã§å‡ºåŠ›ï¼‰

ä½¿ã„æ–¹ä¾‹:
  python tools/append_series_url_from_web.force_dom.py \
    --input data/autohome_raw_2025-08.csv \
    --output data/autohome_raw_2025-08_with_series.csv \
    --rank-url https://www.autohome.com.cn/rank/1-3-1071-x/

"""

import argparse
import re
import sys
from typing import List, Optional

import pandas as pd

# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨
import requests
from bs4 import BeautifulSoup

# Playwright ã¯ä»»æ„ï¼ˆimport å¤±æ•—æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸€æœ¬ã«ï¼‰
try:
    from playwright.sync_api import sync_playwright
    HAS_PW = True
except Exception:
    HAS_PW = False


URL_PATTERN = re.compile(r"https?://www\.autohome\.com\.cn/(\d+)/")


def _extract_series_urls_from_html(html: str) -> List[str]:
    """HTML ã‹ã‚‰ã‚·ãƒªãƒ¼ã‚ºè©³ç´°URLã‚’é †åºä»˜ãã§é‡è¤‡æ’é™¤ã—ã¦æŠ½å‡º"""
    urls = []
    seen = set()
    # ã¾ãšæ­£è¦è¡¨ç¾ã§æ‹¾ã†ï¼ˆé †åºç¶­æŒï¼‰
    for m in URL_PATTERN.finditer(html):
        url = f"https://www.autohome.com.cn/{m.group(1)}/"
        if url not in seen:
            seen.add(url)
            urls.append(url)

    # ã‚»ãƒ¬ã‚¯ã‚¿ã§ã‚‚ä¸€å¿œæ‹¾ã†ï¼ˆé †ç•ªã¯ DOM é †ï¼‰
    try:
        soup = BeautifulSoup(html, "lxml")
        for a in soup.find_all("a", href=True):
            href = a["href"]
            m = URL_PATTERN.match(href)
            if m:
                url = f"https://www.autohome.com.cn/{m.group(1)}/"
                if url not in seen:
                    seen.add(url)
                    urls.append(url)
    except Exception:
        pass

    return urls


def _scrape_with_playwright(url: str, timeout_ms: int = 12000) -> Optional[str]:
    """Playwright ã§ãƒšãƒ¼ã‚¸HTMLã‚’å–å¾—ï¼ˆå¤±æ•—æ™‚ã¯ Noneï¼‰"""
    if not HAS_PW:
        return None
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            ctx = browser.new_context(user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
            ))
            page = ctx.new_page()
            page.set_default_timeout(timeout_ms)
            page.goto(url, wait_until="load")
            # JSãƒ¬ãƒ³ãƒ€å¾…ã¡ã®ä½™ç™½ï¼ˆè»½ã‚ï¼‰
            page.wait_for_timeout(800)
            html = page.content()
            browser.close()
            return html
    except Exception as e:
        print(f"âš  Playwright failed: {e}", file=sys.stderr)
        return None


def _scrape_with_requests(url: str, timeout_s: int = 12) -> Optional[str]:
    """requests ã§ãƒšãƒ¼ã‚¸HTMLã‚’å–å¾—ï¼ˆå¤±æ•—æ™‚ã¯ Noneï¼‰"""
    try:
        headers = {
            "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                           "AppleWebKit/537.36 (KHTML, like Gecko) "
                           "Chrome/124.0.0.0 Safari/537.36")
        }
        r = requests.get(url, headers=headers, timeout=timeout_s)
        if r.status_code == 200 and r.text:
            return r.text
        print(f"âš  requests got status={r.status_code}", file=sys.stderr)
        return None
    except Exception as e:
        print(f"âš  requests failed: {e}", file=sys.stderr)
        return None


def build_web_df(rank_url: str) -> pd.DataFrame:
    """rankãƒšãƒ¼ã‚¸ã‹ã‚‰ [rank, series_url] ã®DFã‚’ä½œã‚‹ã€‚ç©ºã§ã‚‚è¿”ã™ã€‚"""
    html = _scrape_with_playwright(rank_url) or _scrape_with_requests(rank_url)
    if not html:
        print("âš  Unable to fetch HTML from rank_url", file=sys.stderr)
        return pd.DataFrame(columns=["rank", "series_url"])

    urls = _extract_series_urls_from_html(html)
    if not urls:
        print("âš  No series urls found in HTML", file=sys.stderr)
        return pd.DataFrame(columns=["rank", "series_url"])

    # æœ€åˆã®50ä»¶ã‚’ rank=1.. ã¨ã—ã¦æ¡ç•ªï¼ˆå¿…è¦ãªã‚‰æ•°ã¯è‡ªå‹•ã§å¢—ãˆã‚‹ï¼‰
    data = [{"rank": i + 1, "series_url": u} for i, u in enumerate(urls)]
    return pd.DataFrame(data)


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", required=True, help="å…¥åŠ›CSV")
    ap.add_argument("--output", required=True, help="å‡ºåŠ›CSV")
    ap.add_argument("--rank-url",
                    default="https://www.autohome.com.cn/rank/1-3-1071-x/",
                    help="ã‚ªãƒ¼ãƒˆãƒ›ãƒ¼ãƒ ã®ãƒ©ãƒ³ã‚¯ãƒšãƒ¼ã‚¸URL")
    args = ap.parse_args()

    print(f"ğŸ“¥ input: {args.input}")
    print(f"ğŸŒ scraping: {args.rank_url}")

    df = pd.read_csv(args.input)

    # å…¥åŠ›å´ã‚­ãƒ¼ã‚’è‡ªå‹•åˆ¤å®š
    left_key = "rank" if "rank" in df.columns else ("rank_seq" if "rank_seq" in df.columns else None)
    if left_key is None:
        print("âš  input has no 'rank' nor 'rank_seq' â€” will add blank series_url and exit.")
        if "series_url" not in df.columns:
            df["series_url"] = None
        df.to_csv(args.output, index=False)
        return

    # æ–‡å­—â†’æ•°å€¤ã¸ï¼ˆæ··å…¥å¯¾ç­–ï¼‰
    df[left_key] = pd.to_numeric(df[left_key], errors="coerce").astype("Int64")

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    web = build_web_df(args.rank_url)

    # çµæœãŒç©º or rankæ¬ æãªã‚‰ç´ é€šã—
    if web.empty or "rank" not in web.columns:
        print("âš  scraped 'web' has no usable rank; keep input and add missing series_url as NA.")
        if "series_url" not in df.columns:
            df["series_url"] = None
        df.to_csv(args.output, index=False)
        return

    # ãƒãƒ¼ã‚¸ï¼ˆå³å´ã®rankã‚’æ•°å€¤åŒ–ï¼‰
    web["rank"] = pd.to_numeric(web["rank"], errors="coerce").astype("Int64")

    out = df.merge(web[["rank", "series_url"]],
                   left_on=left_key, right_on="rank", how="left")

    # rank_x / rank_y ã®å¾Œå§‹æœ«
    if "rank_y" in out.columns:
        out = out.drop(columns=["rank_y"])
        if "rank_x" in out.columns and left_key == "rank":
            out = out.rename(columns={"rank_x": "rank"})
        elif "rank_x" in out.columns and left_key == "rank_seq":
            # å…¥åŠ›ã®rank_seqã¯ä¿æŒã€rank_xã¯ä¸è¦
            out = out.drop(columns=["rank_x"])

    out.to_csv(args.output, index=False)
    print(f"ğŸ’¾ wrote: {args.output}")


if __name__ == "__main__":
    main()
