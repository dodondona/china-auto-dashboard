# -*- coding: utf-8 -*-
# tools/stage_translate_maker_to_ja.py
#
# ç›®çš„:
#   - 'manufacturer'åˆ—ã‚’æ—¥æœ¬èªåŒ–ã—ã¦'manufacturer_ja'åˆ—ã‚’è¿½åŠ 
#   - 'name'åˆ—ã®éš£ã«'global_name'åˆ—ã‚’è¿½åŠ 
#   - global_nameã¯è¾æ›¸å„ªå…ˆã€ãªã‘ã‚Œã°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€æœ€å¾Œã«LLMç¿»è¨³
#   - æ—¢å­˜å‹•ä½œãƒ»å‡ºåŠ›æ§‹é€ ã¯å¤‰æ›´ã—ãªã„
#
# ä½¿ã„æ–¹:
#   python tools/stage_translate_maker_to_ja.py <csv>

import os, sys, re, json, time
from pathlib import Path
import pandas as pd
from openai import OpenAI

try:
    sys.stdout.reconfigure(line_buffering=True)
except Exception:
    pass

# ==== ãƒ¡ãƒ¼ã‚«ãƒ¼ç¿»è¨³è¾æ›¸ ====
DICT_ZH_TO_JA = {
    # âœ… è‡ªä¸»ãƒ–ãƒ©ãƒ³ãƒ‰
    "æ¯”äºšè¿ª": "BYD",
    "å‰åˆ©": "å‰åˆ©ï¼ˆGeelyï¼‰",
    "å‰åˆ©é“¶æ²³": "å‰åˆ©éŠ€æ²³ï¼ˆGeely Galaxyï¼‰",
    "å¥‡ç‘": "å¥‡ç‘ï¼ˆCheryï¼‰",
    "å¥‡ç‘é£äº‘": "å¥‡ç‘é¢¨é›²ï¼ˆChery Fengyunï¼‰",
    "é•¿å®‰": "é•·å®‰ï¼ˆChanganï¼‰",
    "é•¿å®‰å¯æº": "é•·å®‰å•“æºï¼ˆChangan Qiyuanï¼‰",
    "å“ˆå¼—": "å“ˆå¼—ï¼ˆHavalï¼‰",
    "é­ç‰Œ": "é­ç‰Œï¼ˆWEYï¼‰",
    "çº¢æ——": "ç´…æ——ï¼ˆHongqiï¼‰",
    "åçˆµ": "åçˆµï¼ˆMGï¼‰",
    "è£å¨": "æ „å¨ï¼ˆRoeweï¼‰",
    "é›¶è·‘æ±½è½¦": "é›¶è·‘ï¼ˆLeapmotorï¼‰",
    "ç†æƒ³æ±½è½¦": "ç†æƒ³ï¼ˆLi Autoï¼‰",
    "å°é¹": "å°éµ¬ï¼ˆXpengï¼‰",
    "æç‹": "æ¥µç‹ï¼ˆARCFOXï¼‰",
    "æ·±è“æ±½è½¦": "æ·±è—ï¼ˆDeepalï¼‰",
    "é¢†å…‹": "ãƒªãƒ³ã‚¯ãƒ»ã‚¢ãƒ³ãƒ‰ãƒ»ã‚³ãƒ¼ï¼ˆLynk & Coï¼‰",
    "ä¹é“": "æ¥½é“ï¼ˆLe Daoï¼‰",
    "æ–¹ç¨‹è±¹": "æ–¹ç¨‹è±¹ï¼ˆFang Cheng Baoï¼‰",
    "iCAR": "iCARï¼ˆå¥‡ç‘iCARï¼‰",
    "è…¾åŠ¿": "é¨°å‹¢ï¼ˆDENZAï¼‰",
    "ARCFOX": "æ¥µç‹ï¼ˆARCFOXï¼‰",

    # âœ… ä¸Šæ±½ã‚°ãƒ«ãƒ¼ãƒ—ç³»
    "ä¸Šæ±½": "ä¸Šæµ·æ±½è»Šï¼ˆSAICï¼‰",
    "ä¸Šæ±½é›†å›¢": "ä¸Šæµ·æ±½è»Šï¼ˆSAICï¼‰",
    "ä¸Šæ±½é€šç”¨": "ä¸Šæ±½é€šç”¨ï¼ˆSAIC-GMï¼‰",
    "ä¸Šæ±½é€šç”¨äº”è±": "ä¸Šæ±½é€šç”¨äº”è±ï¼ˆSGMWï¼äº”è±ï¼‰",
    "äº”è±æ±½è½¦": "äº”è±ï¼ˆWulingï¼‰",
    "å®éª": "å®é§¿ï¼ˆBaojunï¼‰",

    # âœ… å¤–è³‡ç³»åˆå¼
    "å¤§ä¼—": "ãƒ•ã‚©ãƒ«ã‚¯ã‚¹ãƒ¯ãƒ¼ã‚²ãƒ³ï¼ˆVolkswagenï¼‰",
    "å¥¥è¿ª": "ã‚¢ã‚¦ãƒ‡ã‚£ï¼ˆAudiï¼‰",
    "å®é©¬": "BMW",
    "å¥”é©°": "ãƒ¡ãƒ«ã‚»ãƒ‡ã‚¹ãƒ»ãƒ™ãƒ³ãƒ„ï¼ˆMercedes-Benzï¼‰",
    "ä¸°ç”°": "ãƒˆãƒ¨ã‚¿ï¼ˆToyotaï¼‰",
    "æœ¬ç”°": "ãƒ›ãƒ³ãƒ€ï¼ˆHondaï¼‰",
    "æ—¥äº§": "æ—¥ç”£ï¼ˆNissanï¼‰",
    "é©¬è‡ªè¾¾": "ãƒãƒ„ãƒ€ï¼ˆMazdaï¼‰",
    "ä¸‰è±": "ä¸‰è±ï¼ˆMitsubishiï¼‰",
    "é“ƒæœ¨": "ã‚¹ã‚ºã‚­ï¼ˆSuzukiï¼‰",
    "æ–¯å·´é²": "ã‚¹ãƒãƒ«ï¼ˆSubaruï¼‰",
    "é›·å…‹è¨æ–¯": "ãƒ¬ã‚¯ã‚µã‚¹ï¼ˆLexusï¼‰",
    "åˆ«å…‹": "ãƒ“ãƒ¥ã‚¤ãƒƒã‚¯ï¼ˆBuickï¼‰",
    "é›ªä½›å…°": "ã‚·ãƒœãƒ¬ãƒ¼ï¼ˆChevroletï¼‰",
    "æ·é€”": "æ·é€”ï¼ˆJetourï¼‰",
    "å¥”è…¾": "å¥”é¨°ï¼ˆBestuneï¼‰",

    # âœ… æ–°èˆˆãŠã‚ˆã³å¤–è³‡ç‹¬è³‡
    "ç‰¹æ–¯æ‹‰": "ãƒ†ã‚¹ãƒ©ï¼ˆTeslaï¼‰",
    "å°ç±³æ±½è½¦": "å°ç±³ï¼ˆXiaomi Autoï¼‰",
    "AITO é—®ç•Œ": "AITOï¼ˆå•ç•Œï¼‰",
    "ARCFOXæç‹": "æ¥µç‹ï¼ˆARCFOXï¼‰",
    "æ–¹ç¨‹è±¹æ±½è½¦": "æ–¹ç¨‹è±¹ï¼ˆFang Cheng Baoï¼‰",
    "å“ˆå¼—çŒ›é¾™æ–°èƒ½æº": "å“ˆå¼—ï¼ˆHavalï¼‰",
    "æ·±è“": "æ·±è—ï¼ˆDeepalï¼‰",
    "é“¶æ²³": "éŠ€æ²³ï¼ˆGeely Galaxyï¼‰",
    "å¯æº": "å•“æºï¼ˆChangan Qiyuanï¼‰",
}

# ==== OpenAI Translator ====
class Translator:
    def __init__(self, model: str, api_key: str | None):
        self.model = model
        self.client = OpenAI(api_key=api_key) if api_key else None
        self.batch_size = 60
        self.retries = 3
        self.sleep_base = 1.2

    def translate_unique(self, terms: list[str]) -> dict[str, str]:
        if not self.client:
            print("âš ï¸ No OpenAI API key; skipping LLM translation")
            return {t: t for t in terms}
        
        result = {}
        for i in range(0, len(terms), self.batch_size):
            batch = terms[i:i + self.batch_size]
            for attempt in range(self.retries):
                try:
                    prompt = self._build_prompt(batch)
                    resp = self.client.chat.completions.create(
                        model=self.model,
                        messages=[{"role": "user", "content": prompt}],
                        temperature=0.3,
                    )
                    content = resp.choices[0].message.content or ""
                    parsed = self._parse_response(content, batch)
                    result.update(parsed)
                    break
                except Exception as e:
                    print(f"âš ï¸ LLM translation attempt {attempt+1}/{self.retries} failed: {e}")
                    if attempt < self.retries - 1:
                        time.sleep(self.sleep_base ** (attempt + 1))
                    else:
                        for t in batch:
                            result[t] = t
        return result

    def _build_prompt(self, terms: list[str]) -> str:
        lines = "\n".join(f"{i+1}. {t}" for i, t in enumerate(terms))
        return f"""ä»¥ä¸‹ã®ä¸­å›½èªã®è‡ªå‹•è»Šãƒ¡ãƒ¼ã‚«ãƒ¼åã¾ãŸã¯è»Šåã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚
å¯èƒ½ã§ã‚ã‚Œã°æ—¥æœ¬èªåã¨è‹±èªè¡¨è¨˜ã‚’ä½µè¨˜ã—ã¦ãã ã•ã„ï¼ˆä¾‹: ãƒˆãƒ¨ã‚¿ï¼ˆToyotaï¼‰ï¼‰ã€‚
å…ƒã®ä¸­å›½èªãŒæ—¢ã«è‹±èªã‚„ãƒ­ãƒ¼ãƒå­—ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã—ã¦ãã ã•ã„ã€‚

å…¥åŠ›:
{lines}

å‡ºåŠ›å½¢å¼ï¼ˆç•ªå·: ç¿»è¨³çµæœï¼‰:
1. ç¿»è¨³çµæœ1
2. ç¿»è¨³çµæœ2
..."""

    def _parse_response(self, content: str, batch: list[str]) -> dict[str, str]:
        result = {}
        lines = [ln.strip() for ln in content.split("\n") if ln.strip()]
        for i, line in enumerate(lines):
            m = re.match(r"^\d+[\.\)]\s*(.+)$", line)
            if m and i < len(batch):
                result[batch[i]] = m.group(1).strip()
        # Fill missing translations
        for t in batch:
            if t not in result:
                result[t] = t
        return result

# ==== ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç† ====
CACHE_DIR = Path("cache/translations")

def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

CACHE_FILES = {
    "manufacturer": CACHE_DIR / "manufacturer_ja.json",
    "vehicle_name": CACHE_DIR / "vehicle_name_ja.json",
}

for cf in CACHE_FILES.values():
    ensure_dir(cf.parent)

def load_json(p: Path) -> dict[str, str]:
    try:
        if p.exists():
            with p.open("r", encoding="utf-8") as f:
                return json.load(f)
    except Exception as e:
        print(f"âš ï¸ cache load failed {p}: {e}")
    return {}

def dump_json_safe(p: Path, data: dict[str, str]):
    try:
        ensure_dir(p.parent)
        tmp = p.with_suffix(p.suffix + ".tmp")
        with tmp.open("w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        tmp.replace(p)
    except Exception as e:
        print(f"âš ï¸ cache save failed {p}: {e}")

# ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
MEM_CACHE = {
    "manufacturer": {},
    "vehicle_name": {},
}

# JSONã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿
JSON_CACHE = {
    "manufacturer": load_json(CACHE_FILES["manufacturer"]),
    "vehicle_name": load_json(CACHE_FILES["vehicle_name"]),
}

def translate_with_caches(kind: str, terms: list[str], fixed_map: dict[str, str], tr: Translator) -> dict[str, str]:
    """
    å„ªå…ˆé †: å›ºå®šè¾æ›¸ > JSONã‚­ãƒ£ãƒƒã‚·ãƒ¥ > ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ > LLM
    """
    out: dict[str, str] = {}

    # 1) å›ºå®šè¾æ›¸
    for t in terms:
        if t in fixed_map:
            out[t] = fixed_map[t]

    # 2) JSONã‚­ãƒ£ãƒƒã‚·ãƒ¥
    for t in terms:
        if t not in out and t in JSON_CACHE[kind]:
            out[t] = JSON_CACHE[kind][t]

    # 3) ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
    for t in terms:
        if t not in out and t in MEM_CACHE[kind]:
            out[t] = MEM_CACHE[kind][t]

    # 4) LLM
    need = [t for t in terms if t not in out]
    if need:
        print(f"ğŸ¤– Translating {len(need)} {kind}(s) with LLM...")
        llm_map = tr.translate_unique(need)
        out.update(llm_map)
        # ãƒ¡ãƒ¢ãƒªãƒ»JSONã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«åæ˜ 
        for k, v in llm_map.items():
            MEM_CACHE[kind][k] = v
            JSON_CACHE[kind][k] = v

    return out

DICT_KEYS_SORTED = sorted(DICT_ZH_TO_JA.keys(), key=len, reverse=True)

# ==== ã‚°ãƒ­ãƒ¼ãƒãƒ«åè¾æ›¸ ====
# ==== ã‚°ãƒ­ãƒ¼ãƒãƒ«åè¾æ›¸ ====
DICT_GLOBAL_NAME = {
    # å‰10ä½
    "å®å…‰MINIEV": "å®å…‰MINIEVï¼ˆHongguang MINI EVï¼‰",
    "Model Y": "ãƒ¢ãƒ‡ãƒ«Y",
    "æ˜Ÿæ„¿": "æ˜Ÿé¡˜ï¼ˆXingyuanï¼‰",
    "ç§¦PLUS": "ç§¦PLUS",
    "è½©é€¸": "ã‚·ãƒ«ãƒ•ã‚£",
    "æµ·ç‹®06æ–°èƒ½æº": "æµ·ç‹®06æ–°èƒ½æºï¼ˆHaishi 06 EVï¼‰",
    "åšè¶ŠL": "åšè¶ŠLï¼ˆBoyue Lï¼‰",
    "æµ·è±¹06æ–°èƒ½æº": "æµ·è±¹06æ–°èƒ½æºï¼ˆHaibao 06 EVï¼‰",
    "ç§¦L": "ç§¦L",
    "å…ƒUP": "å…ƒUP",

    # 11â€“20
    "æµ·é¸¥": "ã‚·ãƒ¼ã‚¬ãƒ«",
    "é€Ÿè…¾": "ã‚µã‚®ã‚¿ãƒ¼ï¼ˆSagitarï¼‰",
    "é•¿å®‰Lumin": "ãƒ«ãƒŸãƒ³ï¼ˆLuminï¼‰",
    "å°ç±³YU7": "YU7",
    "æœ—é€¸": "ãƒ©ãƒ´ã‚£ãƒ¼ãƒ€",
    "æµ·è±š": "ãƒ‰ãƒ«ãƒ•ã‚£ãƒ³",
    "é—®ç•ŒM8": "å•ç•ŒM8ï¼ˆAITO M8ï¼‰",
    "å‡¯ç¾ç‘": "ã‚«ãƒ ãƒª",
    "Model 3": "ãƒ¢ãƒ‡ãƒ«3",
    "RAV4è£æ”¾": "RAV4",

    # 21â€“40
    "å°ç±³SU7": "SU7",
    "é€”è§‚L": "ãƒ†ã‚£ã‚°ã‚¢ãƒ³L",
    "å¸•è¨ç‰¹": "ãƒ‘ã‚µãƒ¼ãƒˆ",
    "é€¸åŠ¨": "é€¸å‹•ï¼ˆYidongï¼‰",
    "æ˜Ÿè¶ŠL": "æ˜Ÿè¶ŠLï¼ˆXingyue Lï¼‰",
    "è¿ˆè…¾": "ãƒã‚´ã‚¿ãƒ³",
    "å“ˆå¼—å¤§ç‹—": "ãƒ“ãƒƒã‚°ãƒ‰ãƒƒã‚°ï¼ˆBig Dogï¼‰",
    "å¥¥è¿ªA6L": "A6L",
    "æ¢å²³": "ã‚¿ã‚¤ãƒ­ãƒ³ï¼ˆTayronï¼‰",
    "å¡ç½—æ‹‰é”æ”¾": "ã‚«ãƒ­ãƒ¼ãƒ©ã‚¯ãƒ­ã‚¹",

    # 41â€“60
    "ç‘è™8": "ãƒ†ã‚£ã‚´8ï¼ˆTiggo 8ï¼‰",
    "å°é¹MONA M03": "MONA M03",
    "æœ¬ç”°CR-V": "CR-V",
    "çº¢æ——H5": "H5",
    "ç¼¤è¶Š": "ã‚¯ãƒ¼ãƒ«ãƒ¬ã‚¤ï¼ˆCoolrayï¼‰",
    "é”‹å…°è¾¾": "ãƒ•ãƒ­ãƒ³ãƒˆãƒ©ãƒ³ãƒ€ãƒ¼",
    "è‰¾ç‘æ³½8": "ã‚¢ãƒªã‚¾8ï¼ˆArrizo 8ï¼‰",
    "å®‹Proæ–°èƒ½æº": "å®‹Proæ–°èƒ½æºï¼ˆSong Pro EVï¼‰",
    "é›…é˜": "ã‚¢ã‚³ãƒ¼ãƒ‰",
    "æ·±è“S05": "æ·±è—S05ï¼ˆDeepal S05ï¼‰",
    "å¥”é©°Eçº§": "Eã‚¯ãƒ©ã‚¹",
    "ç†ŠçŒ«": "ãƒ‘ãƒ³ãƒ€",
    "é“¶æ²³A7": "éŠ€æ²³A7",
    "æ˜‚ç§‘å¨Plus": "ã‚¨ãƒ³ãƒ“ã‚¸ãƒ§ãƒ³Plusï¼ˆEnvision Plusï¼‰",
    "é›¶è·‘C10": "C10",
    "å…ƒPLUS": "ã‚¢ãƒƒãƒˆ3ï¼ˆAtto 3ï¼‰",
    "æµ·è±¹05 DM-i": "ã‚·ãƒ¼ãƒ«05 DM-iï¼ˆSeal 05 DM-iï¼‰",
    "é›¶è·‘B01": "B01",
    "å®é©¬3ç³»": "3ã‚·ãƒªãƒ¼ã‚º",
    "é€”å²³": "é€”å²³ï¼ˆTharuï¼‰",

    # 61â€“80
    "å¥”è…¾å°é©¬": "ãƒãƒ‹ãƒ¼ï¼ˆPonyï¼‰",
    "ç†æƒ³L6": "L6",
    "å¥¥è¿ªQ5L": "Q5L",
    "å¨å…°è¾¾": "ã‚¦ã‚£ãƒ©ãƒ³ãƒ€ãƒ¼",
    "æµ·ç‹®05 EV": "æµ·ç‹®05 EVï¼ˆHaishi 05 EVï¼‰",
    "é•¿å®‰CS75PLUS": "CS75ãƒ—ãƒ©ã‚¹",
    "MG4": "MG4",
    "äºšæ´²é¾™": "ã‚¢ãƒãƒ­ãƒ³",
    "å¥”é©°GLC": "GLC",
    "å“ˆå¼—çŒ›é¾™æ–°èƒ½æº": "ãƒ©ãƒ—ã‚¿ãƒ¼ï¼ˆHaval Raptorï¼‰",
    "å®‹PLUSæ–°èƒ½æº": "å®‹PLUSæ–°èƒ½æºï¼ˆSong PLUS EVï¼‰",
    "ä¹é“L90": "L90ï¼ˆLe Dao L90ï¼‰",
    "é›¶è·‘C11": "C11",
    "é—®ç•ŒM9": "å•ç•ŒM9ï¼ˆAITO M9ï¼‰",
    "å¥”é©°Cçº§": "Cã‚¯ãƒ©ã‚¹",
    "é•¿å®‰å¯æºQ07": "å•“æºQ07ï¼ˆQiyuan Q07ï¼‰",
    "æ·é€”X70": "X70ï¼ˆJetour X70ï¼‰",
    "é“¶æ²³E5": "éŠ€æ²³E5",
    "å®‹L DM-i": "å®‹L DM-i",
    "æç‹T1": "æ¥µç‹T1ï¼ˆARCFOX T1ï¼‰",

    # 81â€“100
    "é“¶æ²³æ˜Ÿè€€8": "éŠ€æ²³æ˜Ÿè€€8",
    "é£äº‘A9L": "é¢¨é›²A9L",
    "çš“å½±": "ãƒ–ãƒªãƒ¼ã‚º",
    "äº”è±ç¼¤æœ": "ãƒ“ãƒ³ã‚´ï¼ˆBingoï¼‰",
    "é›¶è·‘B10": "B10",
    "é•¿å®‰X5 PLUS": "X5ãƒ—ãƒ©ã‚¹",
    "é›¶è·‘C16": "C16",
    "å®é©¬5ç³»": "5ã‚·ãƒªãƒ¼ã‚º",
    "é“‚æ™º3X": "ãƒãƒ«ãƒ3Xï¼ˆbZ3Xï¼‰",
    "è£å¨i5": "i5",
    "é“¶æ²³æ˜Ÿèˆ°7": "éŠ€æ²³æ˜Ÿè‰¦7",
    "èµ›é‚£SIENNA": "ã‚·ã‚¨ãƒŠï¼ˆSiennaï¼‰",
    "é’›7": "ãƒ¬ãƒ‘ãƒ¼ãƒ‰7ï¼ˆLeopard 7ï¼‰",
    "å°é¹P7": "P7",
    "å®é©¬X3": "X3",
    "é•¿å®‰UNI-Zæ–°èƒ½æº": "UNI-Zæ–°èƒ½æº",
    "é­ç‰Œ é«˜å±±": "é«˜å±±ï¼ˆWey Gaoshanï¼‰",
    "iCAR è¶…çº§V23": "iCAR V23",
    "å¥¥è¿ªA4L": "A4L",
    "çº¢æ——HS5": "HS5",
    "é€å®¢": "ã‚­ãƒ£ã‚·ãƒ¥ã‚«ã‚¤",
    "é¢†å…‹900": "ãƒªãƒ³ã‚¯ãƒ»ã‚¢ãƒ³ãƒ‰ãƒ»ã‚³ãƒ¼ 09ï¼ˆLynk & Co 09ï¼‰",
    "æ˜Ÿç‘": "ãƒ—ãƒ¬ãƒ•ã‚§ã‚¤ã‚¹ï¼ˆPrefaceï¼‰",
    "è…¾åŠ¿D9": "ãƒ‡ãƒ³ãƒ„ã‚¡D9ï¼ˆDenza D9ï¼‰",
    "é©±é€èˆ°05": "ãƒ‡ã‚¹ãƒˆãƒ­ã‚¤ãƒ¤ãƒ¼05ï¼ˆDestroyer 05ï¼‰",
    "å¡ç½—æ‹‰": "ã‚«ãƒ­ãƒ¼ãƒ©",
    "åˆ«å…‹GL8æ–°èƒ½æº": "GL8",
    "å®æ¥": "ãƒœãƒ¼ãƒ©ï¼ˆBoraï¼‰",
    "ä¼ ç¥ºGS3": "GS3ï¼ˆTrumpchi GS3ï¼‰",
}
}

# ==== ãƒ”ãƒ³ã‚¤ãƒ³è£œåŠ© ====
try:
    from pypinyin import lazy_pinyin
    _PINYIN_OK = True
except Exception:
    _PINYIN_OK = False

_HAN = r"\u4e00-\u9fff"

def add_block_pinyin_inline(name: str, global_name: str) -> str:
    if re.search(r"[A-Za-zï½-ï½šï¼¡-ï¼ºã‚¡-ãƒ´ãƒ¼]", global_name or ""):
        return global_name
    if global_name or not re.search(fr"[{_HAN}]", name or ""):
        return global_name or name
    if not _PINYIN_OK:
        return name
    s = str(name)
    out = []
    i = 0
    while i < len(s):
        if re.match(fr"[{_HAN}]", s[i]):
            j = i
            while j < len(s) and re.match(fr"[{_HAN}]", s[j]):
                j += 1
            block = s[i:j]
            py = " ".join(lazy_pinyin(block))
            out.append(f"{block}({py})")
            i = j
        else:
            out.append(s[i])
            i += 1
    return "".join(out)

# ==== ãƒ¡ã‚¤ãƒ³ ====
def process_csv(csv_path: Path) -> Path | None:
    print(f"\n=== Processing {csv_path} ===")
    try:
        df = pd.read_csv(csv_path)
    except Exception as e:
        print(f"âš ï¸ cannot read CSV: {e}")
        return None
    if "manufacturer" not in df.columns or "name" not in df.columns:
        print("â„¹ï¸ skip (no 'manufacturer' or 'name')")
        return None

    # OpenAIè¨­å®š
    model = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")
    api_key = os.environ.get("OPENAI_API_KEY")
    tr = Translator(model, api_key)

    # manufacturer_ja - è¾æ›¸å„ªå…ˆã€ãªã‘ã‚Œã°LLM
    print("\nğŸ“‹ Translating manufacturers...")
    uniq_makers = list(set(df["manufacturer"].dropna().astype(str).unique()))
    
    # ã¾ãšè¾æ›¸ã§ãƒãƒƒãƒãƒ³ã‚°ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
    maker_ja_map = {}
    for val in uniq_makers:
        matched = next((DICT_ZH_TO_JA[k] for k in DICT_KEYS_SORTED if k in val), None)
        if matched:
            maker_ja_map[val] = matched
    
    # è¾æ›¸ã«ãªã„ã‚‚ã®ã‚’LLMã§ç¿»è¨³
    need_llm_makers = [m for m in uniq_makers if m not in maker_ja_map]
    if need_llm_makers:
        llm_maker_map = translate_with_caches("manufacturer", need_llm_makers, {}, tr)
        maker_ja_map.update(llm_maker_map)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«é©ç”¨
    df["manufacturer_ja"] = df["manufacturer"].astype(str).map(lambda x: maker_ja_map.get(x, x))

    # global_name - è¾æ›¸å„ªå…ˆã€ãªã‘ã‚Œã°LLMã€æœ€å¾Œã«ãƒ”ãƒ³ã‚¤ãƒ³
    print("\nğŸ“‹ Translating vehicle names...")
    uniq_names = list(set(df["name"].dropna().astype(str).unique()))
    
    # å›ºå®šè¾æ›¸ã‹ã‚‰ãƒãƒƒãƒãƒ³ã‚°
    name_map = {}
    for n in uniq_names:
        if n in DICT_GLOBAL_NAME:
            name_map[n] = DICT_GLOBAL_NAME[n]
    
    # è¾æ›¸ã«ãªã„ã‚‚ã®ã‚’LLMã§ç¿»è¨³
    need_llm_names = [n for n in uniq_names if n not in name_map]
    if need_llm_names:
        llm_name_map = translate_with_caches("vehicle_name", need_llm_names, DICT_GLOBAL_NAME, tr)
        name_map.update(llm_name_map)
    
    # ãƒ”ãƒ³ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆLLMã§ç¿»è¨³ã§ããªã‹ã£ãŸã€ã¾ãŸã¯ä¸­å›½èªã®ã¿ã®å ´åˆï¼‰
    globals_ = []
    for n in df["name"].astype(str):
        g = name_map.get(n, "")
        # ä¸­å›½èªã®ã¿ã®å ´åˆã¯ãƒ”ãƒ³ã‚¤ãƒ³ã‚’è¿½åŠ 
        if not g or (g == n and re.search(r"[\u4e00-\u9fff]", g) and not re.search(r"[A-Za-z]", g)):
            g = add_block_pinyin_inline(n, g)
        globals_.append(g)
    
    insert_at = df.columns.get_loc("name") + 1
    df.insert(insert_at, "global_name", globals_)

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
    dump_json_safe(CACHE_FILES["manufacturer"], JSON_CACHE["manufacturer"])
    dump_json_safe(CACHE_FILES["vehicle_name"], JSON_CACHE["vehicle_name"])

    # âœ… ãƒ•ã‚¡ã‚¤ãƒ«åä¿®æ­£ï¼šæœ«å°¾ã® _with_maker ã‚’1å›ã ã‘é™¤å»
    base = re.sub(r"_with_maker$", "", csv_path.stem)
    out = csv_path.with_name(base + "_with_maker_with_maker_ja.csv")

    df.to_csv(out, index=False, encoding="utf-8-sig")
    print(f"âœ… saved: {out}  rows={len(df)}")
    return out

def main():
    if len(sys.argv) < 2:
        print("Usage: python tools/stage_translate_maker_to_ja.py <csv>")
        sys.exit(1)
    for arg in sys.argv[1:]:
        p = Path(arg)
        if p.exists() and p.suffix.lower() == ".csv":
            process_csv(p)

if __name__ == "__main__":
    main()
